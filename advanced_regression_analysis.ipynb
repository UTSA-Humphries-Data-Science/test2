{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882475da",
   "metadata": {},
   "source": [
    "# Introduction to R Programming - Data Cleaning and Management\n",
    "\n",
    "This section covers fundamental R programming concepts including:\n",
    "- Importing data from different file formats (CSV, TXT, XLSX)\n",
    "- Exploring datasets\n",
    "- Getting summary statistics\n",
    "- Performing arithmetic operations\n",
    "- Recoding variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb1dd3",
   "metadata": {},
   "source": [
    "## Method 1: Importing Data Using Working Directory\n",
    "\n",
    "Set a working directory and import data from that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b79b6c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Setup working directory\n",
    "setwd(\"/workspaces/MS3313_base_template/data/module_1\")\n",
    "\n",
    "# List all files in the working directory\n",
    "list.files()\n",
    "\n",
    "# Import CSV file from working directory\n",
    "directory_data1 <- read.csv(\"Session1-iris.csv\")\n",
    "head(directory_data1)\n",
    "\n",
    "# Import TXT file from working directory\n",
    "directory_data2 <- read.table(\"Session1-iris.txt\")\n",
    "head(directory_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98937a8f",
   "metadata": {},
   "source": [
    "### Importing Excel Files\n",
    "\n",
    "For Excel files (.xlsx), install and use the `readxl` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8c38b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install readxl package if needed\n",
    "# install.packages(\"readxl\")\n",
    "library(\"readxl\")\n",
    "\n",
    "# Import Excel file\n",
    "# directory_data3 <- read_excel(\"your_file.xlsx\")\n",
    "\n",
    "print(\"readxl library loaded for importing Excel files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36beea7",
   "metadata": {},
   "source": [
    "## Data Import Methods: Building Reproducible Workflows\n",
    "\n",
    "### Technical Overview\n",
    "R provides multiple methods for importing data, each with specific use cases:\n",
    "- **`setwd()`**: Sets the working directory, establishing a base path for file operations\n",
    "- **`read.csv()`**: Imports CSV files with comma delimiters\n",
    "- **`read.table()`**: Flexible import function for various delimited files\n",
    "- **`read_excel()`**: Imports Excel files (.xlsx, .xls) - requires `readxl` package\n",
    "\n",
    "### Business Importance\n",
    "**Reproducibility** is critical in business analytics. When you share analysis with stakeholders, they need to:\n",
    "- Re-run your analysis on updated data\n",
    "- Audit your methodology for compliance\n",
    "- Build on your work for future projects\n",
    "\n",
    "**Use absolute paths** (as shown below) to ensure your code works across different systems and users. This is especially important in:\n",
    "- **Collaborative teams**: Multiple analysts working on shared projects\n",
    "- **Production environments**: Automated reports that run on scheduled basis\n",
    "- **Regulatory compliance**: Financial services, healthcare where audit trails are required\n",
    "\n",
    "### When to Use Each Method\n",
    "- **CSV files** (`read.csv`): Most common format for sharing data, universal compatibility\n",
    "- **Text files** (`read.table`): When you have custom delimiters (tabs, pipes, spaces)\n",
    "- **Excel files** (`read_excel`): When data comes from business users who work primarily in Excel\n",
    "\n",
    "**Best Practice**: Always inspect imported data immediately using `head()`, `str()`, `summary()` to catch import errors early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cdcca0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Direct import with full path - TXT file\n",
    "direct_import1 <- read.table(\"/workspaces/MS3313_base_template/data/module_1/Session1-iris.txt\", header=T)\n",
    "head(direct_import1)\n",
    "\n",
    "# Direct import with full path - CSV file\n",
    "direct_import2 <- read.csv(\"/workspaces/MS3313_base_template/data/module_1/Session1-data_corr_mkt.csv\", header=T)\n",
    "head(direct_import2)\n",
    "\n",
    "# For Excel files (uncomment if you have .xlsx files)\n",
    "# direct_import3 <- read_excel(\"/workspaces/MS3313_base_template/data/module_1/your_file.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a91f7",
   "metadata": {},
   "source": [
    "## Data Exploration: The Foundation of Quality Analysis\n",
    "\n",
    "### Technical Overview\n",
    "Before any analysis, you must understand your data structure and quality. Key R functions:\n",
    "- **`View()`**: Opens interactive data viewer (spreadsheet-like interface)\n",
    "- **`names()`**: Lists all column names (verify expected variables exist)\n",
    "- **`dim()`**: Returns dimensions (rows × columns) - check data size\n",
    "- **`str()`**: Shows data structure (data types, sample values) - most comprehensive overview\n",
    "- **`head()` / `tail()`**: Display first/last 6 rows - quick data preview\n",
    "\n",
    "### Business Importance\n",
    "**Data quality issues cost organizations millions annually**. Common problems caught during exploration:\n",
    "- **Missing values**: Revenue data with NAs could skew forecasts\n",
    "- **Wrong data types**: Numeric customer IDs imported as numbers (should be character/factor)\n",
    "- **Duplicates**: Same transaction recorded multiple times inflates sales figures\n",
    "- **Outliers**: $1M transaction in dataset of $100 purchases - data entry error or fraud?\n",
    "\n",
    "### Real-World Applications\n",
    "**Financial Services**: Before building credit risk models, analysts verify:\n",
    "- All expected customer demographics are present (age, income, credit history)\n",
    "- Default indicators are coded correctly (0/1, not Yes/No strings)\n",
    "- No missing values in critical fields that would disqualify loan applications\n",
    "\n",
    "**Retail Analytics**: Marketing teams check:\n",
    "- Sales data has correct date ranges (no future dates, no dates before store opening)\n",
    "- Product categories match company taxonomy\n",
    "- Customer IDs are consistent across purchase records\n",
    "\n",
    "**Rule of Thumb**: Spend 20-30% of analysis time on data exploration. Finding issues early prevents costly errors in final deliverables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ea81f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset for exploration\n",
    "direct_import2 <- read.csv(\"/workspaces/MS3313_base_template/data/module_1/Session1-data_corr_mkt.csv\", header=T)\n",
    "\n",
    "# View the entire dataset (opens in viewer)\n",
    "# View(direct_import2)  # Uncomment to view in RStudio or Jupyter\n",
    "\n",
    "# Get variable names\n",
    "cat(\"Variable names:\\n\")\n",
    "names(direct_import2)\n",
    "\n",
    "# Get dimensions (rows and columns)\n",
    "cat(\"\\nDimensions (rows, columns):\\n\")\n",
    "dim(direct_import2)\n",
    "\n",
    "# Get structure of the dataset\n",
    "cat(\"\\nData structure:\\n\")\n",
    "str(direct_import2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf674713",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get data type/class\n",
    "cat(\"Data class:\\n\")\n",
    "class(direct_import2)\n",
    "\n",
    "# Display first rows\n",
    "cat(\"\\nFirst 5 rows:\\n\")\n",
    "head(direct_import2, n = 5)\n",
    "\n",
    "# Display last rows\n",
    "cat(\"\\nLast 5 rows:\\n\")\n",
    "tail(direct_import2, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6c784",
   "metadata": {},
   "source": [
    "## Summary Statistics: Understanding Data Distributions\n",
    "\n",
    "### Technical Overview\n",
    "R provides multiple packages for comprehensive descriptive statistics:\n",
    "- **Base R `summary()`**: Quick 5-number summary + mean for numeric variables\n",
    "- **`fivenum()`**: Min, Q1, Median, Q3, Max (Tukey's five-number summary)\n",
    "- **`Hmisc::describe()`**: Extensive stats including distinct values, missing counts, quantiles\n",
    "- **`pastecs::stat.desc()`**: Full descriptive stats with variance, CV, SE, confidence intervals\n",
    "- **`psych::describe()`**: Psychology-focused stats with trimmed means, skewness, kurtosis\n",
    "\n",
    "### Business Importance\n",
    "**Understanding distributions is essential for data-driven decisions**:\n",
    "\n",
    "1. **Outlier Detection**: Identify unusual values that could indicate:\n",
    "   - Data entry errors (e.g., salary = $10,000,000 instead of $100,000)\n",
    "   - Fraud or anomalies (e.g., credit card charge 10x customer's typical spending)\n",
    "   - Exceptional cases requiring special handling (e.g., VIP customers with unique behavior)\n",
    "\n",
    "2. **Data Quality Assessment**: \n",
    "   - High standard deviations suggest inconsistent data or need for segmentation\n",
    "   - Extreme skewness indicates log transformations may be needed for modeling\n",
    "   - Missing value counts reveal if imputation strategies are required\n",
    "\n",
    "3. **Business Insights**:\n",
    "   - **Mean vs. Median**: If median income << mean income, few high earners skew average (important for pricing strategies)\n",
    "   - **Quartiles**: Q1 and Q3 help segment customers (low/medium/high value)\n",
    "   - **Range**: Understand variability in KPIs (sales, response times, defect rates)\n",
    "\n",
    "### Real-World Applications\n",
    "**E-Commerce**: Before A/B testing checkout flow, analyze baseline metrics:\n",
    "- Median cart value (better than mean if outliers exist)\n",
    "- 95th percentile checkout time (identify slow experiences)\n",
    "- Skewness in purchase frequency (few power users vs. many casual shoppers)\n",
    "\n",
    "**HR Analytics**: Compensation analysis requires:\n",
    "- Distribution of salaries by role (ensure equitable pay)\n",
    "- Identify outliers (very high/low performers or data errors)\n",
    "- Quartiles for defining pay bands\n",
    "\n",
    "**Best Practice**: Always report median alongside mean for skewed distributions (common in business data like income, transaction size, customer lifetime value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d71f8e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install packages for summary statistics (run once)\n",
    "install.packages(\"Hmisc\")   # Contains many functions for data analysis\n",
    "install.packages(\"pastecs\")  # For space-time series analysis\n",
    "install.packages(\"psych\")    # For comprehensive descriptive statistics\n",
    "\n",
    "# Load libraries\n",
    "library(Hmisc)\n",
    "library(pastecs)\n",
    "library(psych)\n",
    "\n",
    "print(\"Statistical packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8338bee",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Basic summary statistics for entire dataset\n",
    "cat(\"Summary of entire dataset:\\n\")\n",
    "summary(direct_import2)\n",
    "\n",
    "# Summary statistics for a specific variable\n",
    "cat(\"\\n\\nSummary of Sales variable:\\n\")\n",
    "summary(direct_import2$Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c23ad",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Five-number summary (min, Q1, median, Q3, max)\n",
    "# The lower hinge is the smallest value larger than Q1\n",
    "# The upper hinge is the largest value smaller than Q3\n",
    "cat(\"Five-number summary of Sales:\\n\")\n",
    "fivenum(direct_import2$Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47714ce2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Detailed statistics using describe() from Hmisc package\n",
    "# Provides: n, missing, mean, SD, trimmed mean, MAD, skewness, kurtosis, SE\n",
    "cat(\"Detailed description from Hmisc:\\n\")\n",
    "describe(direct_import2$Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c99e8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive statistics using stat.desc() from pastecs package\n",
    "# Provides: variance, coefficient of variation, confidence interval of mean\n",
    "cat(\"Comprehensive statistics from pastecs:\\n\")\n",
    "stat.desc(direct_import2$Sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127e1f8",
   "metadata": {},
   "source": [
    "## Arithmetic Operations: Feature Engineering Fundamentals\n",
    "\n",
    "### Technical Overview\n",
    "R provides three methods to create new variables from existing ones:\n",
    "\n",
    "**Method 1: Direct Assignment**\n",
    "```r\n",
    "data$new_var <- data$var1 + data$var2\n",
    "```\n",
    "- Explicit and clear, shows exact calculation\n",
    "- Best for permanent transformations\n",
    "\n",
    "**Method 2: Using `attach()` and `detach()`**\n",
    "```r\n",
    "attach(data)\n",
    "new_var <- var1 + var2\n",
    "detach(data)\n",
    "```\n",
    "- Simplifies syntax for multiple operations\n",
    "- **Warning**: Can cause naming conflicts in complex scripts\n",
    "\n",
    "**Method 3: Using `transform()`**\n",
    "```r\n",
    "data <- transform(data, new_var = var1 + var2)\n",
    "```\n",
    "- Functional approach, good for creating multiple variables at once\n",
    "- Keeps data manipulation explicit and traceable\n",
    "\n",
    "### Business Importance\n",
    "**Feature engineering creates variables that capture business logic**:\n",
    "\n",
    "1. **Financial Ratios**: \n",
    "   - `profit_margin = (revenue - cost) / revenue`\n",
    "   - `debt_to_equity = total_debt / total_equity`\n",
    "   - These derived metrics are more meaningful than raw numbers for decision-making\n",
    "\n",
    "2. **Customer Metrics**:\n",
    "   - `avg_order_value = total_revenue / num_orders`\n",
    "   - `customer_lifetime_value = avg_order_value * purchase_frequency * customer_lifespan`\n",
    "   - `churn_risk_score = function(recency, frequency, monetary_value)`\n",
    "\n",
    "3. **Operational KPIs**:\n",
    "   - `efficiency_ratio = output_quantity / input_hours`\n",
    "   - `defect_rate = defective_units / total_units`\n",
    "   - `capacity_utilization = actual_output / max_capacity`\n",
    "\n",
    "### Real-World Applications\n",
    "**Credit Risk Modeling**: Banks create dozens of derived features:\n",
    "- `credit_utilization = credit_balance / credit_limit` (strong predictor of default)\n",
    "- `payment_ratio = monthly_payment / monthly_income` (affordability check)\n",
    "- `account_age_months = current_date - account_open_date`\n",
    "\n",
    "**Marketing Analytics**: Campaign performance metrics:\n",
    "- `conversion_rate = conversions / impressions`\n",
    "- `cost_per_acquisition = total_spend / total_conversions`\n",
    "- `ROAS = revenue_from_campaign / campaign_cost` (Return on Ad Spend)\n",
    "\n",
    "**Supply Chain**: Inventory optimization:\n",
    "- `inventory_turnover = cost_of_goods_sold / average_inventory`\n",
    "- `days_sales_outstanding = (accounts_receivable / revenue) * 365`\n",
    "\n",
    "**Best Practice**: Document all derived variables clearly. Future analysts (including your future self!) need to understand the business logic behind each calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955fb04",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "direct_import2 <- read.csv(\"/workspaces/MS3313_base_template/data/module_1/Session1-data_corr_mkt.csv\", header=T)\n",
    "\n",
    "# View variable names\n",
    "cat(\"Original variables:\\n\")\n",
    "names(direct_import2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903e130",
   "metadata": {},
   "source": [
    "## Variable Recoding with attach/detach: Simplifying Syntax\n",
    "\n",
    "### Technical Overview\n",
    "The `attach()`/`detach()` pattern allows you to reference variables without the `data$` prefix:\n",
    "```r\n",
    "attach(data)       # Makes variables directly accessible\n",
    "new_var <- var1 + var2\n",
    "detach(data)       # Removes from search path\n",
    "```\n",
    "\n",
    "**Important**: This is a convenience feature but has risks:\n",
    "- Can create naming conflicts if you have variables with same names in global environment\n",
    "- Less explicit than `data$variable` syntax\n",
    "- Harder to debug in complex scripts with multiple datasets\n",
    "\n",
    "### When to Use\n",
    "- Quick exploratory analysis with single dataset\n",
    "- Interactive console work\n",
    "- Simple scripts where clarity won't suffer\n",
    "\n",
    "### When to Avoid\n",
    "- Production code or complex analysis scripts\n",
    "- When working with multiple datasets simultaneously\n",
    "- Team projects where explicit code is valued\n",
    "\n",
    "**Modern Best Practice**: Most data scientists now prefer `dplyr` verbs or explicit `data$variable` notation over `attach()` for code clarity and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d60e7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Method 1: Direct column assignment using $\n",
    "direct_import2$sum <- direct_import2$Price + direct_import2$Promotion\n",
    "\n",
    "cat(\"Variables after Method 1:\\n\")\n",
    "names(direct_import2)\n",
    "head(direct_import2[, c(\"Price\", \"Promotion\", \"sum\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da663c",
   "metadata": {},
   "source": [
    "## Using transform(): Functional Data Manipulation\n",
    "\n",
    "### Technical Overview\n",
    "`transform()` provides a cleaner way to create multiple new variables:\n",
    "```r\n",
    "data <- transform(data, \n",
    "                  new_var1 = var1 + var2,\n",
    "                  new_var2 = var1 * var3,\n",
    "                  new_var3 = log(var4))\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- All transformations in one call\n",
    "- Returns modified dataset (functional programming style)\n",
    "- More readable than multiple assignment statements\n",
    "\n",
    "### Business Application\n",
    "**Customer Segmentation**: Transform raw transaction data into analytical features:\n",
    "```r\n",
    "customers <- transform(customers,\n",
    "    avg_order_value = total_revenue / num_orders,\n",
    "    recency_days = as.numeric(Sys.Date() - last_purchase_date),\n",
    "    frequency_per_month = num_orders / months_since_first_purchase,\n",
    "    ltv_estimate = avg_order_value * frequency_per_month * 12\n",
    ")\n",
    "```\n",
    "\n",
    "These derived metrics enable RFM (Recency, Frequency, Monetary) segmentation for targeted marketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99021710",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Method 2: Using attach() and detach()\n",
    "attach(direct_import2)\n",
    "direct_import2$sum1 <- Price + Promotion\n",
    "direct_import2$mean1 <- (Price + Promotion) / 2\n",
    "detach(direct_import2)\n",
    "\n",
    "cat(\"Variables after Method 2:\\n\")\n",
    "names(direct_import2)\n",
    "head(direct_import2[, c(\"Price\", \"Promotion\", \"sum1\", \"mean1\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc1d10",
   "metadata": {},
   "source": [
    "## Creating Dummy Variables with ifelse(): Binary Categorization\n",
    "\n",
    "### Technical Overview\n",
    "`ifelse(condition, value_if_true, value_if_false)` creates binary variables based on conditions:\n",
    "```r\n",
    "data$high_price <- ifelse(data$Price > 150, 1, 0)\n",
    "```\n",
    "\n",
    "**Common Use Cases**:\n",
    "- Convert continuous variables to binary flags\n",
    "- Recode categorical variables\n",
    "- Handle missing values with conditions\n",
    "\n",
    "### Business Importance\n",
    "**Dummy variables are essential for regression analysis and machine learning**:\n",
    "\n",
    "1. **Threshold-Based Segmentation**:\n",
    "   - `is_high_value <- ifelse(customer_ltv > 1000, 1, 0)`\n",
    "   - `is_at_risk <- ifelse(days_since_purchase > 90, 1, 0)`\n",
    "   - `is_profitable <- ifelse(revenue > cost, 1, 0)`\n",
    "\n",
    "2. **Regulatory Compliance**:\n",
    "   - `needs_kyc_review <- ifelse(transaction_amount > 10000, 1, 0)` (Anti-money laundering)\n",
    "   - `requires_approval <- ifelse(discount_pct > 30, 1, 0)` (Pricing controls)\n",
    "\n",
    "3. **A/B Testing**:\n",
    "   - `is_treatment <- ifelse(customer_id %% 2 == 0, 1, 0)` (Random assignment)\n",
    "   - `converted <- ifelse(purchased_within_30_days == TRUE, 1, 0)` (Outcome measurement)\n",
    "\n",
    "### Real-World Example\n",
    "**Credit Scoring**: Banks create risk indicators:\n",
    "```r\n",
    "credit_data <- transform(credit_data,\n",
    "    high_utilization = ifelse(balance / credit_limit > 0.8, 1, 0),\n",
    "    recent_delinquency = ifelse(days_since_late_payment < 180, 1, 0),\n",
    "    low_income = ifelse(annual_income < 30000, 1, 0)\n",
    ")\n",
    "```\n",
    "These binary variables become predictors in default probability models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bfde9b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Method 3: Using transform()\n",
    "data4 <- transform(direct_import2,\n",
    "                   sum2 = Price + Promotion,\n",
    "                   mean2 = (Price + Promotion) / 2)\n",
    "\n",
    "cat(\"Variables after Method 3:\\n\")\n",
    "names(data4)\n",
    "head(data4[, c(\"Price\", \"Promotion\", \"sum2\", \"mean2\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db5b3fc",
   "metadata": {},
   "source": [
    "## Creating Categorical Variables with cut(): Binning Continuous Data\n",
    "\n",
    "### Technical Overview\n",
    "`cut()` divides continuous variables into discrete bins/categories:\n",
    "```r\n",
    "data$category <- cut(data$numeric_var, \n",
    "                     breaks = c(0, 50, 100, 150, Inf),\n",
    "                     labels = c(\"Low\", \"Medium\", \"High\", \"Very High\"))\n",
    "```\n",
    "\n",
    "**Key Parameters**:\n",
    "- `breaks`: Boundary values defining bins\n",
    "- `labels`: Names for each category\n",
    "- `right`: Whether intervals are closed on right (default TRUE)\n",
    "- `include.lowest`: Include lowest value in first interval\n",
    "\n",
    "### Business Importance\n",
    "**Categorical variables enable segmentation and interpretable insights**:\n",
    "\n",
    "1. **Customer Segmentation**:\n",
    "   - Age groups: 18-25, 26-35, 36-50, 51+\n",
    "   - Income brackets: <$50K, $50K-$100K, $100K-$200K, $200K+\n",
    "   - Purchase frequency: Occasional, Regular, Frequent, Power User\n",
    "\n",
    "2. **Risk Stratification**:\n",
    "   - Credit scores: Poor (<580), Fair (580-669), Good (670-739), Excellent (740+)\n",
    "   - Claim amounts: Small, Medium, Large, Catastrophic\n",
    "   - Portfolio volatility: Low-risk, Medium-risk, High-risk\n",
    "\n",
    "3. **Pricing Tiers**:\n",
    "   - Usage-based pricing: Free (0-100 API calls), Basic (101-1000), Pro (1001-10000), Enterprise (10000+)\n",
    "   - Shipping costs by weight: Light, Medium, Heavy, Oversized\n",
    "\n",
    "### Real-World Example\n",
    "**E-Commerce Personalization**:\n",
    "```r\n",
    "customers <- transform(customers,\n",
    "    value_segment = cut(lifetime_value,\n",
    "                       breaks = c(0, 100, 500, 2000, Inf),\n",
    "                       labels = c(\"Bronze\", \"Silver\", \"Gold\", \"Platinum\")),\n",
    "    engagement_tier = cut(days_since_purchase,\n",
    "                         breaks = c(0, 30, 90, 180, Inf),\n",
    "                         labels = c(\"Active\", \"Warm\", \"Cool\", \"Dormant\"))\n",
    ")\n",
    "```\n",
    "\n",
    "Marketing can then target:\n",
    "- Platinum/Active: Exclusive early access to new products\n",
    "- Gold/Cool: Re-engagement discount campaign\n",
    "- Bronze/Dormant: Win-back email series\n",
    "\n",
    "**Statistical Note**: Binning can reduce noise in data but also loses information. Use when:\n",
    "- Communication/interpretation requires categories\n",
    "- Non-linear relationships exist (binning can capture)\n",
    "- Regulatory/business requirements dictate specific thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2c34f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Check the summary statistics of Price variable\n",
    "cat(\"Summary of Price variable:\\n\")\n",
    "summary(data4$Price)\n",
    "\n",
    "# Create categorical variable based on Price\n",
    "# Example: Create 2 price categories (Low and High) based on median\n",
    "median_price <- median(data4$Price)\n",
    "data4$Price_Category <- ifelse(data4$Price < median_price, \"Low\", \"High\")\n",
    "\n",
    "# View the new categorical variable\n",
    "cat(\"\\n\\nFrequency table of Price_Category:\\n\")\n",
    "table(data4$Price_Category)\n",
    "\n",
    "# Display some examples\n",
    "cat(\"\\n\\nFirst few rows showing Price and Price_Category:\\n\")\n",
    "head(data4[, c(\"Price\", \"Price_Category\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d9e019",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create multiple categories using cut()\n",
    "# Example: Create 3 price categories using quartiles\n",
    "data4$Price_Category_3 <- cut(data4$Price, \n",
    "                               breaks = quantile(data4$Price, probs = c(0, 0.33, 0.67, 1)),\n",
    "                               labels = c(\"Low\", \"Medium\", \"High\"),\n",
    "                               include.lowest = TRUE)\n",
    "\n",
    "# View the distribution\n",
    "cat(\"Frequency table of 3-category Price variable:\\n\")\n",
    "table(data4$Price_Category_3)\n",
    "\n",
    "# Summary\n",
    "cat(\"\\n\\nSummary of recoded variables:\\n\")\n",
    "summary(data4[, c(\"Price\", \"Price_Category\", \"Price_Category_3\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2d08e",
   "metadata": {},
   "source": [
    "## Basic Visualization: Histograms and Correlation Plots\n",
    "\n",
    "### Technical Overview\n",
    "Core visualization functions for exploratory data analysis:\n",
    "- **`hist()`**: Displays distribution of a single variable\n",
    "- **`pairs()` / `plot()`**: Shows relationships between multiple variables\n",
    "- **Correlation heatmaps**: Visual representation of correlation matrices\n",
    "\n",
    "### Business Importance\n",
    "**Visualizations communicate insights faster than tables of numbers**:\n",
    "\n",
    "1. **Distribution Analysis (Histograms)**:\n",
    "   - **Identify skewness**: Right-skewed income data suggests median is better than mean for analysis\n",
    "   - **Spot outliers**: Visual inspection reveals extreme values needing investigation\n",
    "   - **Assess normality**: Many statistical tests assume normal distributions\n",
    "   \n",
    "   **Business Example**: Histogram of customer transaction amounts reveals:\n",
    "   - Most purchases under $50 (impulse buys)\n",
    "   - Few high-value purchases >$500 (considered purchases requiring different marketing)\n",
    "\n",
    "2. **Correlation Analysis**:\n",
    "   - **Multicollinearity detection**: Highly correlated predictors (r > 0.8) cause problems in regression\n",
    "   - **Feature selection**: Identify which variables relate to target outcome\n",
    "   - **Data validation**: Unexpected correlations may indicate data quality issues\n",
    "   \n",
    "   **Business Example**: Marketing spend correlation matrix shows:\n",
    "   - Radio and TV ads highly correlated (0.9) → may activate same campaigns\n",
    "   - Billboard ads weakly correlated with sales (0.2) → questionable ROI\n",
    "   - Need to carefully model individual channel effects due to multicollinearity\n",
    "\n",
    "### Real-World Applications\n",
    "**Financial Services**: \n",
    "- Histogram of loan amounts reveals bimodal distribution (small personal loans vs. large mortgages) → need separate models\n",
    "- Correlation between credit score and default rate informs risk pricing\n",
    "\n",
    "**Supply Chain**:\n",
    "- Histogram of delivery times identifies consistent delays in certain regions\n",
    "- Correlation between supplier lead time and stockouts predicts inventory needs\n",
    "\n",
    "**Best Practice**: Always visualize your data BEFORE modeling. Charts reveal patterns, outliers, and issues that summary statistics might miss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b32ec",
   "metadata": {},
   "source": [
    "## Interactive Data Exploration Tools (Data Wrangler Alternatives)\n",
    "\n",
    "R has several powerful packages for interactive data exploration and visualization, similar to Data Wrangler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a79c621",
   "metadata": {},
   "source": [
    "### 1. DataExplorer - Automated EDA with Interactive Reports\n",
    "\n",
    "Creates comprehensive HTML reports with distributions, correlations, missing data analysis, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5f228",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install DataExplorer package\n",
    "install.packages(\"DataExplorer\")\n",
    "library(DataExplorer)\n",
    "\n",
    "# Load sample data\n",
    "data_for_exploration <- read.csv(\"/workspaces/MS3313_base_template/data/module_1/Session1-data_corr_mkt.csv\", header=T)\n",
    "\n",
    "# Quick overview of the dataset\n",
    "cat(\"=== Data Structure Overview ===\\n\")\n",
    "introduce(data_for_exploration)\n",
    "\n",
    "# Plot introduction (shows basic statistics)\n",
    "plot_intro(data_for_exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ccd69",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "plot_missing(data_for_exploration)\n",
    "\n",
    "# Plot distributions of all features with better sizing\n",
    "# Increase figure size to prevent label overlap\n",
    "options(repr.plot.width=14, repr.plot.height=10)\n",
    "plot_histogram(data_for_exploration)\n",
    "\n",
    "\n",
    "# Plot correlation heatmap with better formattingplot_correlation(data_for_exploration)\n",
    "options(repr.plot.width=12, repr.plot.height=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620173d5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a comprehensive HTML report (like Data Wrangler)\n",
    "# This creates an interactive HTML file with all visualizations\n",
    "create_report(data_for_exploration, \n",
    "              output_file = \"data_exploration_report.html\",\n",
    "              output_dir = \"/workspaces/MS3313_base_template/\")\n",
    "\n",
    "cat(\"Uncomment the code above to generate a full interactive HTML report!\\n\")\n",
    "cat(\"The report will include:\\n\")\n",
    "cat(\"  - Basic statistics\\n\")\n",
    "cat(\"  - Missing data analysis\\n\")\n",
    "cat(\"  - Distribution plots\\n\")\n",
    "cat(\"  - Correlation analysis\\n\")\n",
    "cat(\"  - Principal component analysis\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44dc4d2",
   "metadata": {},
   "source": [
    "### 3. dplyr + tidyverse - Powerful Data Wrangling with Pipes\n",
    "\n",
    "The tidyverse approach provides intuitive, readable data manipulation similar to Data Wrangler's operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2114d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install tidyverse packages\n",
    "# install.packages(\"tidyverse\")\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "\n",
    "# Example: Data wrangling operations using pipe operator %>%\n",
    "wrangled_data <- data_for_exploration %>%\n",
    "  # Filter rows\n",
    "  filter(Sales > mean(Sales, na.rm = TRUE)) %>%\n",
    "  # Select specific columns\n",
    "  select(Sales, Price, Promotion, Distribution) %>%\n",
    "  # Create new variables\n",
    "  mutate(\n",
    "    Sales_Per_Price = Sales / Price,\n",
    "    High_Promotion = ifelse(Promotion > median(Promotion), \"High\", \"Low\")\n",
    "  ) %>%\n",
    "  # Group and summarize\n",
    "  group_by(High_Promotion) %>%\n",
    "  summarise(\n",
    "    Avg_Sales = mean(Sales, na.rm = TRUE),\n",
    "    Avg_Price = mean(Price, na.rm = TRUE),\n",
    "    Count = n()\n",
    "  )\n",
    "\n",
    "print(wrangled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed82c50",
   "metadata": {},
   "source": [
    "### 4. plotly - Interactive Plots\n",
    "\n",
    "Create interactive visualizations that you can zoom, pan, and hover over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d990e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install plotly\n",
    "install.packages(\"plotly\")\n",
    "library(plotly)\n",
    "\n",
    "# Create an interactive scatter plot\n",
    "fig <- plot_ly(data = data_for_exploration, \n",
    "               x = ~Price, \n",
    "               y = ~Sales,\n",
    "               type = 'scatter',\n",
    "               mode = 'markers',\n",
    "               marker = list(size = 10, \n",
    "                           color = ~Promotion,\n",
    "                           colorscale = 'Viridis',\n",
    "                           showscale = TRUE),\n",
    "               text = ~paste('Price:', Price, '<br>Sales:', Sales, '<br>Promotion:', Promotion),\n",
    "               hoverinfo = 'text')\n",
    "\n",
    "fig <- fig %>% layout(title = \"Interactive Sales vs Price\",\n",
    "                      xaxis = list(title = \"Price\"),\n",
    "                      yaxis = list(title = \"Sales\"))\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ccfe03",
   "metadata": {},
   "source": [
    "### Summary: Best Tools for Data Wrangler-Like Experience in Notebooks\n",
    "\n",
    "| Tool | Best For | Output |\n",
    "|------|----------|--------|\n",
    "| **DataExplorer** | Automated EDA reports | Inline plots + HTML reports |\n",
    "| **dplyr/tidyverse** | Data transformation pipelines | Cleaned datasets |\n",
    "| **DT** | Interactive data tables | Interactive HTML tables |\n",
    "| **plotly** | Interactive visualizations | Interactive plots |\n",
    "\n",
    "**Recommended Workflow:**\n",
    "1. Use **DataExplorer** plots for visual exploration\n",
    "2. Use **dplyr** for data cleaning/transformation\n",
    "3. Browse data with **DT** interactive tables\n",
    "4. Create interactive visualizations with **plotly**\n",
    "\n",
    "**Note:** Some R packages (summarytools, skimr) may have compatibility issues in Jupyter notebooks. Stick to the tools listed above for reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df6661",
   "metadata": {},
   "source": [
    "## Using Data Wrangler in VS Code\n",
    "\n",
    "VS Code has a built-in **Data Wrangler** that provides a visual, Excel-like interface for exploring and transforming data. You can use it with R dataframes in Jupyter notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6b626",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load a dataset to explore with Data Wrangler\n",
    "sales_data <- read.csv(\"/workspaces/MS3313_base_template/data/module_1/Session1-data_corr_mkt.csv\", header=T)\n",
    "\n",
    "# Display the data\n",
    "head(sales_data)\n",
    "\n",
    "# After running this cell, you can:\n",
    "# 1. Look for 'sales_data' in the Variables pane\n",
    "# 2. Click on it to open Data Wrangler\n",
    "# 3. Use the visual interface to explore and transform your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699be598",
   "metadata": {},
   "source": [
    "## Alternative: Interactive Data Exploration Tools in R\n",
    "\n",
    "While waiting for Data Wrangler to load, you can also use these R packages for interactive data exploration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DataExplorer - Automated EDA with Interactive Reports\n",
    "\n",
    "Generate comprehensive exploratory data analysis reports automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e100742",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install DataExplorer package\n",
    "install.packages(\"DataExplorer\")\n",
    "library(DataExplorer)\n",
    "\n",
    "# Load example data\n",
    "data_explore <- read.csv(\"/workspaces/MS3313_base_template/data/module_1/Session1-data_corr_mkt.csv\", header=T)\n",
    "\n",
    "# Quick overview of the data\n",
    "cat(\"=== Data Structure Overview ===\\n\")\n",
    "introduce(data_explore)\n",
    "\n",
    "# Create a comprehensive EDA report (opens in browser)\n",
    "create_report(data_explore, y = \"Sales\")  # Uncomment to generate HTML report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf25656",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize data structure\n",
    "plot_intro(data_explore)\n",
    "\n",
    "# Check for missing data\n",
    "plot_missing(data_explore)\n",
    "\n",
    "# View distributions of all continuous variables\n",
    "# Set larger plot size to prevent label overlap\n",
    "options(repr.plot.width=14, repr.plot.height=10)\n",
    "plot_histogram(data_explore)\n",
    "\n",
    "\n",
    "# View correlations with better sizingplot_correlation(data_explore)\n",
    "options(repr.plot.width=12, repr.plot.height=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7940ef",
   "metadata": {},
   "source": [
    "## Interaction Effects: When Variables Work Together\n",
    "\n",
    "### Technical Overview\n",
    "**Interaction terms** capture how the effect of one variable depends on another:\n",
    "\n",
    "**Additive Model**: y = β₀ + β₁x₁ + β₂x₂\n",
    "- Effect of x₁ is constant regardless of x₂ value\n",
    "- Effects are independent\n",
    "\n",
    "**Interaction Model**: y = β₀ + β₁x₁ + β₂x₂ + β₃(x₁ × x₂)\n",
    "- Effect of x₁ changes depending on x₂ level\n",
    "- Variables have synergistic or antagonistic effects\n",
    "\n",
    "```r\n",
    "model_interaction <- lm(y ~ x1 * x2, data = data)\n",
    "# Equivalent to: y ~ x1 + x2 + x1:x2\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- **β₁**: Effect of x₁ when x₂ = 0\n",
    "- **β₂**: Effect of x₂ when x₁ = 0\n",
    "- **β₃**: How much the effect of x₁ changes for each unit increase in x₂\n",
    "\n",
    "### Business Importance\n",
    "**Real business effects are rarely additive - interactions reveal synergies and compound effects**:\n",
    "\n",
    "1. **Marketing Synergies**:\n",
    "   - TV advertising alone: +10% sales\n",
    "   - Digital advertising alone: +8% sales\n",
    "   - TV + Digital together: +25% sales (not just 18%)\n",
    "   - **Interaction effect**: Campaigns reinforce each other\n",
    "\n",
    "2. **Conditional Relationships**:\n",
    "   - Price cuts increase sales, BUT the effect is larger for:\n",
    "     - Well-known brands vs. unknown brands\n",
    "     - Discretionary products vs. necessities\n",
    "     - Price-sensitive customer segments vs. loyal customers\n",
    "   - **Interaction with brand strength, product type, customer segment**\n",
    "\n",
    "3. **Segmentation Insights**:\n",
    "   - A feature may increase satisfaction for one customer segment but decrease for another\n",
    "   - Training effectiveness depends on employee experience level\n",
    "   - Sales tactics work differently by industry/region\n",
    "   - **Interactions reveal where to customize strategies**\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Promotional Effectiveness**:\n",
    "```r\n",
    "sales_model <- lm(sales ~ price * promotion, data = sales_data)\n",
    "```\n",
    "**Interpretation**:\n",
    "- `price` coefficient: Sales impact of $1 price change when NO promotion\n",
    "- `promotion` coefficient: Sales lift from promotion at base price\n",
    "- `price:promotion` coefficient: How promotion changes price sensitivity\n",
    "\n",
    "**Business Scenario**:\n",
    "- Regular periods: 10% price cut → +5% sales\n",
    "- During promotion: 10% price cut → +15% sales\n",
    "- **Insight**: Promotions amplify price elasticity\n",
    "- **Strategy**: Bundle promotions with discounts for maximum impact\n",
    "\n",
    "**Employee Performance**:\n",
    "```r\n",
    "performance_model <- lm(sales ~ experience * training_hours, data = employees)\n",
    "```\n",
    "**Potential Finding**:\n",
    "- New employees (< 1 year): Training highly effective (+$50K sales per 10 hours)\n",
    "- Experienced employees (> 5 years): Training less effective (+$10K sales per 10 hours)\n",
    "- **Business Value**: Allocate training budget to new hires for higher ROI\n",
    "\n",
    "**Product-Market Fit**:\n",
    "```r\n",
    "revenue_model <- lm(revenue ~ product_features * market_segment, data = sales)\n",
    "```\n",
    "**Interpretation**:\n",
    "- Enterprise segment: More features → Higher willingness to pay\n",
    "- SMB segment: More features → Lower adoption (too complex)\n",
    "- **Strategy**: Offer feature-rich version for Enterprise, simplified for SMB\n",
    "\n",
    "**Geographic Pricing**:\n",
    "```r\n",
    "demand_model <- lm(quantity ~ price * region, data = regional_sales)\n",
    "```\n",
    "**Finding**:\n",
    "- Urban regions: High price elasticity (many substitutes)\n",
    "- Rural regions: Low price elasticity (fewer alternatives)\n",
    "- **Business Value**: Implement region-specific pricing strategies\n",
    "\n",
    "### Visualization and Interpretation\n",
    "**Always plot interactions**:\n",
    "```r\n",
    "library(ggplot2)\n",
    "ggplot(data, aes(x = x1, y = y, color = factor(x2))) +\n",
    "  geom_point() +\n",
    "  geom_smooth(method = \"lm\", se = TRUE) +\n",
    "  labs(title = \"Interaction: Effect of x1 varies by x2 level\")\n",
    "```\n",
    "\n",
    "**Look for**:\n",
    "- Non-parallel lines indicate interaction\n",
    "- Crossing lines indicate sign reversal (effect positive for one group, negative for another)\n",
    "\n",
    "### Statistical Testing\n",
    "**Test if interaction is significant**:\n",
    "1. Fit model with interaction\n",
    "2. Check p-value for interaction term\n",
    "3. Compare AIC/BIC to additive model\n",
    "\n",
    "**Caution**: \n",
    "- Interactions reduce degrees of freedom (less statistical power)\n",
    "- Can lead to overfitting with many interactions\n",
    "- Ensure sufficient sample size in each subgroup\n",
    "\n",
    "**Best Practice**:\n",
    "- Start with business hypotheses: \"I expect x₁ effect to differ by x₂\"\n",
    "- Test specific interactions, not all possible combinations\n",
    "- Validate on holdout data\n",
    "- When interaction is significant, report conditional effects separately for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5233bfb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install summarytools\n",
    "install.packages(\"summarytools\")\n",
    "library(summarytools)\n",
    "\n",
    "# Frequency tables for all variables\n",
    "freq(data_explore)\n",
    "\n",
    "# Comprehensive summary with statistics and histograms\n",
    "dfSummary(data_explore)\n",
    "\n",
    "# You can also view in browser:\n",
    "view(dfSummary(data_explore))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74627f32",
   "metadata": {},
   "source": [
    "## Quadratic Terms: Modeling Non-Linear Relationships\n",
    "\n",
    "### Technical Overview\n",
    "**Quadratic (polynomial) regression** captures curved relationships using squared terms:\n",
    "\n",
    "**Linear Model**: y = β₀ + β₁x\n",
    "- Assumes constant rate of change\n",
    "- Straight line relationship\n",
    "\n",
    "**Quadratic Model**: y = β₀ + β₁x + β₂x²\n",
    "- Allows for curvature (U-shaped or inverted-U)\n",
    "- Rate of change varies with x\n",
    "\n",
    "```r\n",
    "model_quadratic <- lm(y ~ x + I(x^2), data = data)\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- If β₂ > 0: U-shaped (parabola opens upward)\n",
    "- If β₂ < 0: Inverted-U (parabola opens downward)\n",
    "- **Turning point**: x = -β₁ / (2β₂)\n",
    "\n",
    "### Business Importance\n",
    "**Many business relationships are non-linear - quadratic models capture diminishing returns and optimal points**:\n",
    "\n",
    "1. **Diminishing Returns**:\n",
    "   - First $100K in marketing yields high returns\n",
    "   - Next $100K yields moderate returns\n",
    "   - Eventually, additional spending has minimal impact\n",
    "   - **Quadratic model identifies the optimal spending level**\n",
    "\n",
    "2. **Optimal Points**:\n",
    "   - **Price optimization**: Too low (low margin), too high (low volume) - sweet spot in middle\n",
    "   - **Staffing levels**: Too few (poor service), too many (high costs) - optimal staffing exists\n",
    "   - **Product features**: Too simple (boring), too complex (confusing) - ideal complexity level\n",
    "\n",
    "3. **Lifecycle Patterns**:\n",
    "   - Product sales: growth phase (accelerating), maturity (decelerating), decline\n",
    "   - Customer satisfaction: increases with response time initially, plateaus, then drops if too slow\n",
    "   - Learning curves: rapid improvement initially, then slowing gains\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Marketing Spend Optimization**:\n",
    "```r\n",
    "model <- lm(sales ~ marketing_spend + I(marketing_spend^2), data = campaigns)\n",
    "summary(model)\n",
    "\n",
    "# Find optimal spend where marginal return = 0\n",
    "# d(sales)/d(spend) = β₁ + 2*β₂*spend = 0\n",
    "optimal_spend <- -coef(model)[2] / (2 * coef(model)[3])\n",
    "```\n",
    "**Business Value**: \n",
    "- If optimal = $500K and currently spending $1M → save $500K with same sales\n",
    "- If spending $200K → increase budget to $500K for significant sales gains\n",
    "\n",
    "**Pricing Strategy**:\n",
    "```r\n",
    "revenue_model <- lm(revenue ~ price + I(price^2), data = sales_history)\n",
    "```\n",
    "**Interpretation**: \n",
    "- Revenue = (price × quantity), but higher prices reduce quantity\n",
    "- Quadratic model finds revenue-maximizing price\n",
    "- **Business Value**: Price too low leaves money on table; too high loses customers\n",
    "\n",
    "**Employee Training ROI**:\n",
    "```r\n",
    "productivity_model <- lm(output_per_hour ~ training_hours + I(training_hours^2))\n",
    "```\n",
    "**Finding**: \n",
    "- Initial training (0-40 hours) has high ROI\n",
    "- Moderate training (40-80 hours) has diminishing returns\n",
    "- Excessive training (>80 hours) reduces productivity (workers away from job)\n",
    "- **Business Value**: Optimal training = 60 hours maximizes output\n",
    "\n",
    "**Customer Service Satisfaction**:\n",
    "```r\n",
    "satisfaction_model <- lm(csat_score ~ response_time_minutes + I(response_time_minutes^2))\n",
    "```\n",
    "**Finding**: \n",
    "- Up to 10 minutes: Satisfaction stable (customers understand wait)\n",
    "- 10-30 minutes: Satisfaction drops moderately\n",
    "- Beyond 30 minutes: Satisfaction plummets (quadratic drop)\n",
    "- **Business Value**: Staffing should target <10 minute response times\n",
    "\n",
    "### Model Validation\n",
    "**Always visualize the quadratic fit**:\n",
    "```r\n",
    "library(ggplot2)\n",
    "ggplot(data, aes(x = x, y = y)) +\n",
    "  geom_point() +\n",
    "  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = TRUE)\n",
    "```\n",
    "\n",
    "**Check for**:\n",
    "- Does the curve make business sense?\n",
    "- Is the turning point within data range? (Don't extrapolate parabolas!)\n",
    "- Compare R² to linear model - significant improvement?\n",
    "\n",
    "**Best Practice**: \n",
    "- Use quadratic terms when you expect optimal points or diminishing returns\n",
    "- Don't use for extrapolation (parabolas can predict nonsensical values outside data range)\n",
    "- Consider interaction terms alongside quadratic terms for complex relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38c02e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install DT package\n",
    "# install.packages(\"DT\")\n",
    "library(DT)\n",
    "\n",
    "# Create an interactive datatable\n",
    "# You can sort, filter, and search through your data\n",
    "datatable(data_explore, \n",
    "          options = list(pageLength = 10, scrollX = TRUE),\n",
    "          filter = 'top')\n",
    "\n",
    "# This creates a fully interactive table where you can:\n",
    "# - Sort columns by clicking headers\n",
    "# - Filter data using the search boxes\n",
    "# - Navigate through pages\n",
    "# - Export data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17ba2e",
   "metadata": {},
   "source": [
    "### Summary: Data Wrangling Tools Comparison\n",
    "\n",
    "| Tool | Best For | Interactive? | Output Type |\n",
    "|------|----------|--------------|-------------|\n",
    "| **VS Code Data Wrangler** | Visual data transformation, Excel-like interface | ✅ Yes | Generates R code |\n",
    "| **DataExplorer** | Automated EDA, comprehensive reports | ✅ Yes | Inline plots + HTML |\n",
    "| **dplyr/tidyverse** | Data transformation pipelines | ❌ No | Cleaned datasets |\n",
    "| **DT** | Interactive data browsing/filtering | ✅ Yes | Interactive HTML table |\n",
    "| **plotly** | Interactive visualizations | ✅ Yes | Interactive plots |\n",
    "\n",
    "**Recommendation:** Use **DataExplorer** for visual exploration, **DT** for interactive data browsing, **plotly** for interactive plots, and **VS Code Data Wrangler** for visual transformations. Use base R functions like `summary()`, `str()`, and `head()` for quick data inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ed9bb",
   "metadata": {},
   "source": [
    "### How to Open Data Wrangler:\n",
    "\n",
    "1. **Run a cell** that creates a dataframe (like the ones below)\n",
    "2. **Click the variable** in the Variables pane (usually on the left side)\n",
    "3. **Click \"Open in Data Wrangler\"** button that appears\n",
    "4. Explore your data with sorting, filtering, and visual transformations\n",
    "5. **Export the code** that Data Wrangler generates back into your notebook\n",
    "\n",
    "Let's create some dataframes you can explore with Data Wrangler:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0e2b3",
   "metadata": {},
   "source": [
    "## VS Code Data Wrangler: Excel-like Data Exploration in R\n",
    "\n",
    "### Technical Overview\n",
    "**Data Wrangler** is a built-in VS Code extension that provides a graphical interface for data exploration and manipulation, similar to Excel pivot tables or Tableau Prep.\n",
    "\n",
    "**Key Features**:\n",
    "- Point-and-click data filtering, sorting, and aggregation\n",
    "- Visual column profiling (distributions, missing values, unique counts)\n",
    "- Generate R/Python code automatically from your GUI interactions\n",
    "- Export cleaned/transformed data back to your workflow\n",
    "\n",
    "**How to Use**:\n",
    "1. Load your dataset in R: `data <- read.csv(\"file.csv\")`\n",
    "2. Click the Data Wrangler icon in VS Code\n",
    "3. Explore data visually with interactive charts and summaries\n",
    "4. Apply transformations using GUI (filter, group, aggregate)\n",
    "5. Export generated code back to your notebook\n",
    "\n",
    "### Business Importance\n",
    "**Bridges the gap between business users and data science**:\n",
    "\n",
    "1. **Faster Exploration**:\n",
    "   - Business analysts familiar with Excel can explore data without learning R syntax\n",
    "   - Data scientists can quickly prototype transformations before writing production code\n",
    "   - Stakeholder demos: Show data interactively during meetings\n",
    "\n",
    "2. **Code Generation**:\n",
    "   - GUI actions automatically generate corresponding R/Python code\n",
    "   - Ensures reproducibility while maintaining ease of use\n",
    "   - Learning tool: See how clicks translate to code\n",
    "\n",
    "3. **Collaboration**:\n",
    "   - Non-technical team members can explore data and share findings\n",
    "   - Data stewards can validate data quality visually\n",
    "   - Reduces back-and-forth between analysts and stakeholders\n",
    "\n",
    "### Real-World Use Cases\n",
    "**Sales Analytics**: \n",
    "- Marketing manager uses Data Wrangler to explore campaign performance\n",
    "- Filters by region, sorts by ROI, identifies top-performing channels\n",
    "- Exports findings; analyst incorporates insights into automated reporting pipeline\n",
    "\n",
    "**Data Quality Auditing**:\n",
    "- Data engineer uses visual profiling to spot missing values, outliers\n",
    "- Quickly identifies which columns need cleaning\n",
    "- Generates code for consistent data validation across datasets\n",
    "\n",
    "**When to Use Data Wrangler vs. Code**:\n",
    "- **Use GUI**: Initial exploration, stakeholder demos, one-off analysis\n",
    "- **Use Code**: Automated pipelines, complex transformations, version control\n",
    "\n",
    "**Best Practice**: Use Data Wrangler for exploration, then copy generated code into your notebook for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ad250",
   "metadata": {},
   "source": [
    "## DataExplorer: Automated Exploratory Data Analysis\n",
    "\n",
    "### Technical Overview\n",
    "**DataExplorer** is an R package that automates EDA with comprehensive visualizations and statistics:\n",
    "```r\n",
    "library(DataExplorer)\n",
    "create_report(data)  # Generates full HTML report\n",
    "plot_missing(data)   # Missing value analysis\n",
    "plot_bar(data)       # Categorical variable distributions\n",
    "plot_histogram(data) # Numeric variable distributions\n",
    "plot_correlation(data) # Correlation heatmap\n",
    "```\n",
    "\n",
    "**Key Functions**:\n",
    "- `introduce()`: Dataset overview (dimensions, data types, missing %)\n",
    "- `plot_intro()`: Visual summary of dataset structure\n",
    "- `plot_missing()`: Missing data patterns\n",
    "- `plot_correlation()`: Correlation matrix heatmap\n",
    "- `create_report()`: One-line comprehensive EDA report\n",
    "\n",
    "### Business Importance\n",
    "**Accelerates the EDA phase from hours to minutes**:\n",
    "\n",
    "1. **Time Efficiency**:\n",
    "   - Manual EDA: 2-4 hours writing custom code for each dataset\n",
    "   - DataExplorer: 5 minutes to generate comprehensive report\n",
    "   - **ROI**: Analysts can explore 10x more hypotheses in same time\n",
    "\n",
    "2. **Standardized Analysis**:\n",
    "   - Every dataset gets same thorough examination\n",
    "   - Reduces risk of missing important patterns\n",
    "   - Consistent format aids communication across teams\n",
    "\n",
    "3. **Stakeholder Communication**:\n",
    "   - HTML reports are shareable with non-technical audiences\n",
    "   - Interactive visualizations explain data structure clearly\n",
    "   - Management can review data quality without R knowledge\n",
    "\n",
    "### Real-World Applications\n",
    "**New Dataset Onboarding**:\n",
    "- Data science team receives customer churn data from client\n",
    "- Run `create_report()` in 5 minutes to understand data structure\n",
    "- Identify: 15% missing values in income, high correlation between tenure and charges\n",
    "- Present findings to client, agree on data cleaning strategy\n",
    "\n",
    "**Data Quality Monitoring**:\n",
    "- Monthly data pipeline delivers sales data\n",
    "- Automated script runs DataExplorer report\n",
    "- Alerts trigger if missing value % increases or distributions shift\n",
    "- Prevents bad data from entering downstream models\n",
    "\n",
    "**Model Feature Selection**:\n",
    "- Before building predictive model, analyze all potential features\n",
    "- `plot_correlation()` reveals multicollinearity issues\n",
    "- `plot_missing()` shows which variables can't be used (too many NAs)\n",
    "- Focus modeling effort on high-quality, relevant features\n",
    "\n",
    "**Best Practice**: Run DataExplorer on every new dataset before analysis. The automated report often reveals insights you wouldn't have thought to check manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c256d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# Uncomment the following lines to install packages\n",
    " install.packages(\"MASS\")\n",
    " install.packages(\"aod\")\n",
    " install.packages(\"ggplot2\")\n",
    " install.packages(\"pscl\")\n",
    "\n",
    "# Load required libraries\n",
    "library(MASS)\n",
    "library(aod)\n",
    "library(ggplot2)\n",
    "library(pscl)\n",
    "\n",
    "print(\"All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b4c6f",
   "metadata": {},
   "source": [
    "## dplyr + tidyverse: Modern Data Wrangling with Pipes\n",
    "\n",
    "### Technical Overview\n",
    "**dplyr** is the industry-standard R package for data manipulation, using intuitive verb-based functions:\n",
    "\n",
    "**Core Verbs**:\n",
    "- `filter()`: Keep rows matching conditions\n",
    "- `select()`: Choose specific columns\n",
    "- `mutate()`: Create new variables or modify existing ones\n",
    "- `summarize()`: Aggregate data (mean, sum, count, etc.)\n",
    "- `group_by()`: Group data for aggregated operations\n",
    "- `arrange()`: Sort rows\n",
    "\n",
    "**Pipe Operator (`%>%`)**: Chains operations together, reading left-to-right:\n",
    "```r\n",
    "data %>%\n",
    "  filter(sales > 1000) %>%\n",
    "  group_by(region) %>%\n",
    "  summarize(avg_sales = mean(sales)) %>%\n",
    "  arrange(desc(avg_sales))\n",
    "```\n",
    "\n",
    "### Business Importance\n",
    "**dplyr is the SQL of R - essential for business analytics**:\n",
    "\n",
    "1. **Readable Code = Maintainable Analysis**:\n",
    "   - Pipes read like business logic: \"Take data, filter high-value customers, group by segment, calculate average\"\n",
    "   - New analysts can understand code without extensive R knowledge\n",
    "   - 6 months later, you'll understand your own code (critical for audit trails)\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - Optimized C++ backend handles millions of rows efficiently\n",
    "   - Lazy evaluation for database connections (works with SQL databases directly)\n",
    "   - Verb-based approach is faster to write than base R subsetting\n",
    "\n",
    "3. **Standardization**:\n",
    "   - Industry standard taught in most data science programs\n",
    "   - Consistent syntax across entire tidyverse ecosystem\n",
    "   - Team collaboration easier when everyone uses same paradigm\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Sales Performance Dashboard**:\n",
    "```r\n",
    "sales_summary <- sales_data %>%\n",
    "  filter(date >= '2024-01-01') %>%\n",
    "  group_by(region, product_category) %>%\n",
    "  summarize(\n",
    "    total_revenue = sum(revenue),\n",
    "    avg_order_value = mean(order_value),\n",
    "    num_transactions = n(),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  arrange(desc(total_revenue))\n",
    "```\n",
    "**Business Value**: Transforms raw transaction log into executive dashboard in 5 lines\n",
    "\n",
    "**Customer Churn Analysis**:\n",
    "```r\n",
    "at_risk_customers <- customers %>%\n",
    "  filter(days_since_purchase > 90) %>%\n",
    "  filter(lifetime_value > 500) %>%\n",
    "  mutate(\n",
    "    churn_risk_score = (days_since_purchase / 365) * (1 / purchase_frequency),\n",
    "    priority = case_when(\n",
    "      churn_risk_score > 0.8 & lifetime_value > 2000 ~ \"High\",\n",
    "      churn_risk_score > 0.5 ~ \"Medium\",\n",
    "      TRUE ~ \"Low\"\n",
    "    )\n",
    "  ) %>%\n",
    "  arrange(desc(lifetime_value))\n",
    "```\n",
    "**Business Value**: Identifies high-value customers at risk of churning for retention campaigns\n",
    "\n",
    "**Marketing Campaign ROI**:\n",
    "```r\n",
    "campaign_performance <- marketing_data %>%\n",
    "  group_by(campaign_name, channel) %>%\n",
    "  summarize(\n",
    "    total_spend = sum(ad_spend),\n",
    "    total_revenue = sum(attributed_revenue),\n",
    "    conversions = sum(conversion_count),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    roas = total_revenue / total_spend,\n",
    "    cost_per_acquisition = total_spend / conversions\n",
    "  ) %>%\n",
    "  filter(roas > 1) %>%  # Only profitable campaigns\n",
    "  arrange(desc(roas))\n",
    "```\n",
    "**Business Value**: Determines which marketing channels deliver positive ROI\n",
    "\n",
    "**Best Practice**: Learn dplyr verbs - they're the foundation of modern R data analysis. Investing time here pays dividends across all future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb73949",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Note: Replace 'data' with your actual dataset\n",
    "# Example: \n",
    "data <- read.csv(\"/workspaces/MS3313_base_template/data/module_1/your_file.csv\", header=T)\n",
    "\n",
    "# Stepwise regression\n",
    "fit <- lm(Sales~Price+Distribution+Promotion+competition+customer_satisfaction+age+total_employee, data=data)\n",
    "step <- stepAIC(fit, direction=\"both\")\n",
    "step$anova # display results\n",
    "\n",
    "print(\"Stepwise regression will be performed once data is loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00111dde",
   "metadata": {},
   "source": [
    "## DT: Interactive HTML Tables for Data Presentation\n",
    "\n",
    "### Technical Overview\n",
    "**DT** (DataTables) creates interactive, searchable, sortable tables in notebooks and web reports:\n",
    "```r\n",
    "library(DT)\n",
    "datatable(data, \n",
    "          filter = 'top',          # Add column filters\n",
    "          options = list(\n",
    "            pageLength = 25,       # Rows per page\n",
    "            scrollX = TRUE,        # Horizontal scrolling for wide tables\n",
    "            order = list(c(2, 'desc'))  # Sort by column 3 descending\n",
    "          ))\n",
    "```\n",
    "\n",
    "**Key Features**:\n",
    "- Sortable columns (click headers)\n",
    "- Search/filter functionality\n",
    "- Pagination for large datasets\n",
    "- Export to CSV/Excel/PDF\n",
    "- Custom formatting (colors, conditional formatting)\n",
    "\n",
    "### Business Importance\n",
    "**Interactive tables bridge the gap between raw data and insights**:\n",
    "\n",
    "1. **Stakeholder Self-Service**:\n",
    "   - Executives can explore data without requesting custom queries\n",
    "   - \"Sort by revenue\" or \"filter to Q4 2024\" without analyst intervention\n",
    "   - Reduces ad-hoc analysis requests\n",
    "\n",
    "2. **Data Validation**:\n",
    "   - Quality assurance teams can quickly scan data for anomalies\n",
    "   - Search for specific customer IDs, product codes, transaction numbers\n",
    "   - Faster than scrolling through static Excel files\n",
    "\n",
    "3. **Interactive Reporting**:\n",
    "   - Embed in RMarkdown reports for interactive dashboards\n",
    "   - Recipients can explore details without reopening R\n",
    "   - Professional presentation for client deliverables\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Sales Territory Review**:\n",
    "```r\n",
    "sales_by_rep <- sales_data %>%\n",
    "  group_by(sales_rep, territory) %>%\n",
    "  summarize(\n",
    "    total_sales = sum(revenue),\n",
    "    num_deals = n(),\n",
    "    avg_deal_size = mean(revenue)\n",
    "  )\n",
    "\n",
    "datatable(sales_by_rep,\n",
    "          filter = 'top',\n",
    "          options = list(pageLength = 50)) %>%\n",
    "  formatCurrency(c('total_sales', 'avg_deal_size'), '$') %>%\n",
    "  formatStyle('total_sales',\n",
    "              backgroundColor = styleInterval(c(100000, 500000),\n",
    "                                            c('red', 'yellow', 'green')))\n",
    "```\n",
    "**Business Value**: Sales managers can interactively explore performance, identify top performers, and spot underperforming territories\n",
    "\n",
    "**Inventory Dashboard**:\n",
    "```r\n",
    "inventory_status <- products %>%\n",
    "  mutate(\n",
    "    stock_status = case_when(\n",
    "      quantity_on_hand == 0 ~ \"Out of Stock\",\n",
    "      quantity_on_hand < reorder_point ~ \"Low Stock\",\n",
    "      TRUE ~ \"In Stock\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "datatable(inventory_status, filter = 'top') %>%\n",
    "  formatStyle('stock_status',\n",
    "              backgroundColor = styleEqual(\n",
    "                c('Out of Stock', 'Low Stock', 'In Stock'),\n",
    "                c('red', 'orange', 'lightgreen')\n",
    "              ))\n",
    "```\n",
    "**Business Value**: Operations team can quickly identify stockouts and reorder priorities\n",
    "\n",
    "**Best Practice**: Use DT for any table you'll share with stakeholders. The interactivity dramatically improves usability compared to static tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc6a95",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Robust regression model (requires MASS package)\n",
    "# fit <- rlm(Sales~Price+Distribution+Promotion+competition+customer_satisfaction+age+total_employee, data=data)\n",
    "# summary(fit)\n",
    "\n",
    "print(\"Robust regression will be performed once data is loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad630f",
   "metadata": {},
   "source": [
    "## plotly: Interactive Visualizations for Exploration\n",
    "\n",
    "### Technical Overview\n",
    "**plotly** creates interactive charts that support zooming, panning, hover tooltips, and filtering:\n",
    "```r\n",
    "library(plotly)\n",
    "\n",
    "# Convert ggplot to interactive plotly\n",
    "p <- ggplot(data, aes(x = var1, y = var2)) + geom_point()\n",
    "ggplotly(p)\n",
    "\n",
    "# Or create plotly directly\n",
    "plot_ly(data, x = ~var1, y = ~var2, type = 'scatter', mode = 'markers')\n",
    "```\n",
    "\n",
    "**Key Features**:\n",
    "- Hover tooltips showing exact values\n",
    "- Click-and-drag to zoom into regions\n",
    "- Double-click to reset view\n",
    "- Toggle legend items to show/hide series\n",
    "- Export chart as PNG from browser\n",
    "\n",
    "### Business Importance\n",
    "**Interactive charts enable deeper data exploration and better communication**:\n",
    "\n",
    "1. **Exploratory Analysis**:\n",
    "   - Zoom into outliers to investigate specific data points\n",
    "   - Hover to see exact values without cluttering axes\n",
    "   - Quickly toggle different series on/off to compare patterns\n",
    "   \n",
    "   **Example**: Time series of daily sales - zoom into specific weeks to investigate anomalies\n",
    "\n",
    "2. **Stakeholder Presentations**:\n",
    "   - Executives can interact with charts during meetings\n",
    "   - \"What was revenue in March?\" → Hover to see exact value\n",
    "   - \"How does Region A compare to B?\" → Click legend to isolate series\n",
    "   \n",
    "   **Business Value**: Reduces need for follow-up questions and custom charts\n",
    "\n",
    "3. **Self-Service Analytics**:\n",
    "   - Embed in dashboards for business users to explore themselves\n",
    "   - Non-technical users can investigate patterns interactively\n",
    "   - Reduces analyst workload for ad-hoc requests\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Sales Funnel Analysis**:\n",
    "```r\n",
    "funnel_data <- data.frame(\n",
    "  stage = c(\"Leads\", \"Qualified\", \"Proposal\", \"Negotiation\", \"Closed\"),\n",
    "  count = c(10000, 5000, 2000, 800, 300)\n",
    ")\n",
    "\n",
    "plot_ly(funnel_data, x = ~count, y = ~stage, type = 'funnel') %>%\n",
    "  layout(title = \"Sales Funnel Conversion\")\n",
    "```\n",
    "**Business Value**: Interactive funnel lets sales managers click stages to drill down\n",
    "\n",
    "**Time Series Dashboard**:\n",
    "```r\n",
    "plot_ly(financial_data, x = ~date, y = ~revenue, type = 'scatter', mode = 'lines+markers',\n",
    "        text = ~paste(\"Date:\", date, \"<br>Revenue:\", revenue, \"<br>Region:\", region),\n",
    "        hoverinfo = 'text') %>%\n",
    "  layout(title = \"Revenue Trends\", \n",
    "         xaxis = list(rangeslider = list(visible = TRUE)))\n",
    "```\n",
    "**Business Value**: Range slider lets users zoom to specific time periods; tooltips show full context\n",
    "\n",
    "**Regional Performance Map** (if geographic data available):\n",
    "```r\n",
    "plot_ly(sales_by_state, type = 'choropleth',\n",
    "        locations = ~state_code, locationmode = 'USA-states',\n",
    "        z = ~total_sales, text = ~state_name,\n",
    "        colorscale = 'Blues') %>%\n",
    "  layout(geo = list(scope = 'usa'))\n",
    "```\n",
    "**Business Value**: Interactive map reveals geographic patterns; hover shows state details\n",
    "\n",
    "**Best Practice**: Use plotly for any chart where stakeholders might want to explore details. The interactivity significantly enhances understanding compared to static images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a9e5e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Model with Quadratic terms\n",
    "# fit <- lm(Sales~Price+I(Price^2)+Distribution+I(Distribution^2)+Promotion+competition+customer_satisfaction+age+total_employee, data=data)\n",
    "# summary(fit)\n",
    "\n",
    "print(\"Quadratic regression model will be fitted once data is loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc435228",
   "metadata": {},
   "source": [
    "## Stepwise Regression: Automated Variable Selection\n",
    "\n",
    "### Technical Overview\n",
    "**Stepwise regression** automatically selects the best subset of predictors using statistical criteria:\n",
    "\n",
    "**Methods**:\n",
    "- **Forward Selection**: Start with no variables, add most significant one at a time\n",
    "- **Backward Elimination**: Start with all variables, remove least significant one at a time\n",
    "- **Both (default)**: Combines forward and backward, can add or remove at each step\n",
    "\n",
    "**Selection Criteria**:\n",
    "- **AIC (Akaike Information Criterion)**: Lower is better, balances model fit and complexity\n",
    "- **BIC (Bayesian Information Criterion)**: More conservative than AIC, penalizes complexity more\n",
    "\n",
    "**R Implementation**:\n",
    "```r\n",
    "library(MASS)\n",
    "full_model <- lm(y ~ x1 + x2 + x3 + x4, data = data)\n",
    "stepwise_model <- stepAIC(full_model, direction = \"both\")\n",
    "```\n",
    "\n",
    "### Business Importance\n",
    "**Prevents model overfitting and identifies truly impactful variables**:\n",
    "\n",
    "1. **Avoiding Overfitting**:\n",
    "   - Including too many predictors creates models that work great on training data but fail on new data\n",
    "   - Stepwise regression finds simpler models with better generalization\n",
    "   - **Business Impact**: Models deployed in production stay accurate over time\n",
    "\n",
    "2. **Resource Optimization**:\n",
    "   - Identifies which data to collect/maintain\n",
    "   - If a variable doesn't improve predictions, stop collecting it (saves costs)\n",
    "   - Focus analysis efforts on variables that matter\n",
    "   \n",
    "   **Example**: Marketing model shows billboard ads don't predict sales → Stop billboard campaigns, reallocate budget\n",
    "\n",
    "3. **Interpretability**:\n",
    "   - Simpler models are easier to explain to stakeholders\n",
    "   - \"Sales depend on price and distribution\" is clearer than 20-variable model\n",
    "   - Regulatory compliance often requires explainable models (fair lending, insurance pricing)\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Credit Risk Modeling**:\n",
    "- Start with 50+ customer attributes (income, age, employment, credit history, etc.)\n",
    "- Stepwise regression identifies 8-12 key predictors of default\n",
    "- **Business Value**: Streamlined data collection for loan applications, faster decisions\n",
    "\n",
    "**Retail Demand Forecasting**:\n",
    "- Potential predictors: price, promotions, seasonality, competitor prices, weather, holidays\n",
    "- Stepwise finds that price, promotions, and month are sufficient\n",
    "- **Business Value**: Simpler inventory planning model that's easier to maintain\n",
    "\n",
    "**Employee Attrition**:\n",
    "- HR has data on 30+ employee characteristics\n",
    "- Stepwise identifies: manager rating, time-since-promotion, commute time as top predictors\n",
    "- **Business Value**: Targeted retention interventions on specific risk factors\n",
    "\n",
    "### Important Caveats\n",
    "**Stepwise is controversial in statistics**:\n",
    "- Can be unstable with collinear predictors\n",
    "- P-values may not be reliable after selection\n",
    "- Alternative: Use domain knowledge + regularization (LASSO) for variable selection\n",
    "\n",
    "**Best Practice**: \n",
    "- Use stepwise for initial exploration, not final conclusions\n",
    "- Validate selected model on holdout data\n",
    "- Consider business logic alongside statistical criteria\n",
    "- Document why variables were included/excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d88e93",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Log-Log Models: Elasticity and Percentage Changes\n",
    "\n",
    "### Technical Overview\n",
    "**Multiplicative (log-log) models** use logarithmic transformations on both dependent and independent variables:\n",
    "\n",
    "**Standard Model**: y = β₀ + β₁x\n",
    "- Coefficients represent absolute changes\n",
    "- \"1 unit increase in x → β₁ unit change in y\"\n",
    "\n",
    "**Log-Log Model**: log(y) = β₀ + β₁log(x)\n",
    "- Coefficients represent **elasticities** (percentage changes)\n",
    "- \"1% increase in x → β₁% change in y\"\n",
    "\n",
    "```r\n",
    "model_loglog <- lm(log(y) ~ log(x1) + log(x2), data = data)\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Coefficient of 0.5 means: 10% increase in x → 5% increase in y\n",
    "- Coefficient of -1.2 means: 10% increase in x → 12% decrease in y\n",
    "- **Units don't matter** - elasticity is unit-free\n",
    "\n",
    "### Business Importance\n",
    "**Elasticities are the language of business economics - executives think in percentages, not absolute units**:\n",
    "\n",
    "1. **Comparable Across Products/Markets**:\n",
    "   - \"Sales drop 10 units when price increases $1\" → Not comparable across products\n",
    "   - \"Sales drop 5% when price increases 10%\" → Universal comparison\n",
    "   - **Business Value**: Compare strategies across product lines, regions, time periods\n",
    "\n",
    "2. **Strategic Decision-Making**:\n",
    "   - **Price Elasticity**: How revenue responds to price changes\n",
    "     - Elastic (|elasticity| > 1): Lower price → Higher revenue\n",
    "     - Inelastic (|elasticity| < 1): Raise price → Higher revenue\n",
    "   - **Income Elasticity**: How demand responds to economic growth\n",
    "   - **Advertising Elasticity**: ROI of marketing investments\n",
    "\n",
    "3. **Executive Communication**:\n",
    "   - \"10% price increase reduces volume by 15% but increases revenue by 2%\"\n",
    "   - Percentages are intuitive for C-suite, board presentations\n",
    "   - Directly links to KPIs (growth rates, margin targets)\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Price Elasticity Analysis**:\n",
    "```r\n",
    "demand_model <- lm(log(quantity) ~ log(price) + log(competitor_price) + log(income), \n",
    "                   data = sales_data)\n",
    "summary(demand_model)\n",
    "```\n",
    "**Interpretation**:\n",
    "- `log(price)` coefficient = -1.8: **Own-price elasticity**\n",
    "  - 10% price increase → 18% quantity decrease\n",
    "  - Revenue = Price × Quantity → Revenue falls 8% (elastic demand)\n",
    "  - **Strategy**: Don't raise prices; consider lowering them\n",
    "  \n",
    "- `log(competitor_price)` coefficient = 0.6: **Cross-price elasticity**\n",
    "  - 10% competitor price increase → 6% increase in our sales\n",
    "  - Products are substitutes (positive elasticity)\n",
    "  - **Strategy**: Monitor competitor pricing closely\n",
    "\n",
    "- `log(income)` coefficient = 1.2: **Income elasticity**\n",
    "  - 10% GDP growth → 12% sales increase\n",
    "  - Luxury good (income elasticity > 1)\n",
    "  - **Strategy**: Target affluent markets; sales vulnerable in recessions\n",
    "\n",
    "**Advertising ROI**:\n",
    "```r\n",
    "sales_model <- lm(log(sales) ~ log(tv_spend) + log(digital_spend) + log(print_spend))\n",
    "```\n",
    "**Finding**:\n",
    "- TV: elasticity = 0.15 (10% increase in TV spend → 1.5% sales increase)\n",
    "- Digital: elasticity = 0.25 (10% increase in digital spend → 2.5% sales increase)\n",
    "- Print: elasticity = 0.05 (10% increase in print spend → 0.5% sales increase)\n",
    "\n",
    "**Business Decision**:\n",
    "- Current budget: TV $1M, Digital $500K, Print $300K\n",
    "- Reallocation: Shift from Print to Digital (higher elasticity)\n",
    "- **Expected Impact**: Same total spend, but 5% sales increase from better allocation\n",
    "\n",
    "**Production Economics**:\n",
    "```r\n",
    "output_model <- lm(log(output) ~ log(labor_hours) + log(capital_investment))\n",
    "```\n",
    "**Interpretation**:\n",
    "- Labor elasticity = 0.7: 10% more labor → 7% more output\n",
    "- Capital elasticity = 0.4: 10% more capital → 4% more output\n",
    "- **Returns to scale** = 0.7 + 0.4 = 1.1\n",
    "  - >1: Increasing returns (economies of scale)\n",
    "  - **Strategy**: Expand operations - bigger is better\n",
    "\n",
    "**Market Sizing**:\n",
    "```r\n",
    "market_model <- lm(log(market_size) ~ log(population) + log(gdp_per_capita))\n",
    "```\n",
    "**Use Case**: Estimate market potential in new geographic regions\n",
    "- Population elasticity = 0.9: Larger cities have proportionally smaller markets (saturation)\n",
    "- GDP elasticity = 1.3: Wealthier regions disproportionately larger markets\n",
    "- **Strategy**: Prioritize wealthy, medium-sized cities over mega-cities\n",
    "\n",
    "### Technical Considerations\n",
    "\n",
    "**When to Use Log-Log Models**:\n",
    "- Variables cover wide ranges (prices $1 to $1000, sales 100 to 100,000)\n",
    "- Business interest in percentage changes\n",
    "- Theoretical expectation of constant elasticity\n",
    "- Multiplicative relationships (y = x₁^β₁ × x₂^β₂)\n",
    "\n",
    "**Limitations**:\n",
    "- Cannot handle zero or negative values (log undefined)\n",
    "  - Solution: Add small constant, use log(1+x), or use different transformation\n",
    "- Assumes constant elasticity (may not hold over full data range)\n",
    "- Interpretation harder for non-experts (requires explaining logs)\n",
    "\n",
    "**Validation**:\n",
    "```r\n",
    "# Check if log transformation improves fit\n",
    "model_linear <- lm(sales ~ price, data = data)\n",
    "model_loglog <- lm(log(sales) ~ log(price), data = data)\n",
    "AIC(model_linear)\n",
    "AIC(model_loglog)  # Lower AIC = better fit\n",
    "\n",
    "# Plot to verify linearity in log-log space\n",
    "plot(log(data$price), log(data$sales))\n",
    "abline(model_loglog)\n",
    "```\n",
    "\n",
    "**Best Practice**:\n",
    "- Report elasticities with confidence intervals\n",
    "- Interpret elasticities in business context (not just \"statistically significant\")\n",
    "- Validate elasticity estimates on holdout periods\n",
    "- Compare to industry benchmarks (e.g., typical price elasticity for your product category)\n",
    "- Document assumptions (constant elasticity, no zero values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0034a1",
   "metadata": {},
   "source": [
    "## Robust Regression: Handling Outliers and Influential Points\n",
    "\n",
    "### Technical Overview\n",
    "**Robust regression** reduces the influence of outliers compared to ordinary least squares (OLS):\n",
    "\n",
    "**Problem with OLS**: Outliers can dramatically skew regression lines because OLS minimizes squared errors (outliers have huge squared errors).\n",
    "\n",
    "**Solution - M-estimation (rlm)**:\n",
    "- Uses iteratively reweighted least squares\n",
    "- Downweights observations with large residuals\n",
    "- Produces coefficient estimates less sensitive to outliers\n",
    "\n",
    "```r\n",
    "library(MASS)\n",
    "robust_model <- rlm(y ~ x1 + x2, data = data, psi = psi.huber)\n",
    "```\n",
    "\n",
    "**Weighting Functions**:\n",
    "- `psi.huber`: Default, balanced approach\n",
    "- `psi.bisquare`: More aggressive outlier downweighting\n",
    "\n",
    "### Business Importance\n",
    "**Real-world data always has outliers - robust methods prevent them from ruining your analysis**:\n",
    "\n",
    "1. **Data Quality Issues**:\n",
    "   - **Data entry errors**: Someone typed $100,000 instead of $10,000 for salary\n",
    "   - **Legitimate extremes**: CEO salary in dataset of employees\n",
    "   - **One-time events**: Pandemic causing sales spike/drop\n",
    "   \n",
    "   OLS treats these equally with normal data; robust regression adapts\n",
    "\n",
    "2. **Better Predictions**:\n",
    "   - Model reflects typical relationships, not distorted by unusual cases\n",
    "   - More reliable forecasts for \"normal\" business conditions\n",
    "   - **Example**: Store sales model not thrown off by Super Bowl Sunday spike\n",
    "\n",
    "3. **Identifying Problematic Data**:\n",
    "   - Cases with low weights in robust regression are flagged for investigation\n",
    "   - May reveal fraud, data errors, or special cases requiring different treatment\n",
    "   - Quality assurance tool for data pipelines\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Pricing Analysis**:\n",
    "```r\n",
    "# Price elasticity model\n",
    "robust_model <- rlm(log(sales) ~ log(price) + promotions + seasonality, data = sales_data)\n",
    "```\n",
    "**Problem**: A few stores had pricing errors (extreme discounts) causing huge sales spikes\n",
    "**Solution**: Robust regression estimates typical price elasticity, ignoring the errors\n",
    "**Business Value**: Pricing team gets reliable elasticity estimates for strategy\n",
    "\n",
    "**Real Estate Valuation**:\n",
    "```r\n",
    "property_model <- rlm(price ~ sqft + bedrooms + bathrooms + age, data = homes)\n",
    "```\n",
    "**Problem**: Dataset includes luxury mansions alongside typical homes\n",
    "**Solution**: Robust regression models typical homes; separate model for luxury segment\n",
    "**Business Value**: Accurate appraisals for 95% of market\n",
    "\n",
    "**Employee Performance**:\n",
    "```r\n",
    "performance_model <- rlm(performance_rating ~ experience + training_hours + projects_completed)\n",
    "```\n",
    "**Problem**: Few superstars and few underperformers skew OLS estimates\n",
    "**Solution**: Robust regression models typical employee-performance relationship\n",
    "**Business Value**: Better guidance for average employee development\n",
    "\n",
    "### Diagnostic Workflow\n",
    "1. **Fit both OLS and robust regression**\n",
    "2. **Compare coefficients**: Large differences indicate outlier influence\n",
    "3. **Examine weights**: Identify low-weight cases for investigation\n",
    "4. **Decide**: Fix data errors, use robust estimates, or segment data\n",
    "\n",
    "**When to Use Robust Regression**:\n",
    "- Known outliers you can't remove (valid but extreme data)\n",
    "- Data quality concerns but can't manually clean\n",
    "- Exploratory analysis to see if outliers drive results\n",
    "- Supplement to OLS, not replacement (report both)\n",
    "\n",
    "**Best Practice**: Always plot residuals. Robust regression helps with outliers but won't fix fundamental model misspecification (non-linear relationships, missing variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd485fb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Multiplicative Model\n",
    "# This is a multiplicative model, we can also insert individual interaction terms\n",
    "# fit3 <- lm(log(data$Sales1)~log(data$Price)+log(data$Distribution)+log(data$total_employee)+log(data$competition), data=data)\n",
    "# summary(fit3)\n",
    "\n",
    "print(\"Multiplicative (log-log) model will be fitted once data is loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25873654",
   "metadata": {},
   "source": [
    "## Logistic Regression: Modeling Binary Outcomes\n",
    "\n",
    "### Technical Overview\n",
    "**Logistic regression** models the probability of binary outcomes (Yes/No, Default/No Default, Buy/Don't Buy):\n",
    "\n",
    "**Why Not Linear Regression?**\n",
    "- Binary outcome (0 or 1) violates linear regression assumptions\n",
    "- Linear regression can predict probabilities < 0 or > 1 (nonsensical)\n",
    "- Relationship between predictors and probability is S-shaped, not linear\n",
    "\n",
    "**Logistic Model**:\n",
    "```\n",
    "P(Y=1) = 1 / (1 + e^-(β₀ + β₁x₁ + β₂x₂ + ...))\n",
    "```\n",
    "\n",
    "**In R**:\n",
    "```r\n",
    "logit_model <- glm(outcome ~ x1 + x2 + x3, data = data, family = \"binomial\")\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Coefficients represent log-odds (not intuitive)\n",
    "- **Odds Ratios** (exponentiated coefficients) are interpretable:\n",
    "  - OR = 1.5: Variable increases odds of outcome by 50%\n",
    "  - OR = 0.8: Variable decreases odds of outcome by 20%\n",
    "  - OR = 1.0: Variable has no effect\n",
    "\n",
    "### Business Importance\n",
    "**Most critical business outcomes are binary - logistic regression is the workhorse of predictive analytics**:\n",
    "\n",
    "1. **Customer Behavior Prediction**:\n",
    "   - Will customer churn? (Yes/No)\n",
    "   - Will prospect convert? (Buy/Don't Buy)\n",
    "   - Will customer click on ad? (Click/No Click)\n",
    "   - Will email be opened? (Open/Ignore)\n",
    "\n",
    "2. **Risk Assessment**:\n",
    "   - Will loan default? (Default/Repay)\n",
    "   - Will insurance claim be fraudulent? (Fraud/Legitimate)\n",
    "   - Will patient have adverse event? (Event/No Event)\n",
    "   - Will project exceed budget? (Overrun/On Budget)\n",
    "\n",
    "3. **Decision Support**:\n",
    "   - Model outputs probability (0-1 scale)\n",
    "   - Business sets threshold based on costs/benefits\n",
    "   - **Example**: Approve loan if default probability < 5%\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Credit Card Default Prediction** (using the dataset in this notebook):\n",
    "```r\n",
    "default_model <- glm(d_dummy ~ student_dummy + balance + income + gender + edu + avg_late_payment,\n",
    "                     data = credit_data, \n",
    "                     family = \"binomial\")\n",
    "```\n",
    "\n",
    "**Variables**:\n",
    "- `d_dummy`: Default indicator (1 = default, 0 = no default)\n",
    "- `student_dummy`: Student status (1 = student, 0 = non-student)\n",
    "- `balance`: Credit card balance\n",
    "- `income`: Annual income\n",
    "- `avg_late_payment`: Average days late on payments\n",
    "\n",
    "**Business Interpretation**:\n",
    "- **Balance (positive coefficient)**: Higher balance → Higher default risk\n",
    "  - OR = 1.005: Each $1000 in balance increases default odds by 5%\n",
    "  - **Action**: Lower credit limits for high-balance customers\n",
    "  \n",
    "- **Income (negative coefficient)**: Higher income → Lower default risk\n",
    "  - OR = 0.998: Each $10K income increase decreases default odds by 2%\n",
    "  - **Action**: Income verification is valuable for risk assessment\n",
    "  \n",
    "- **Late Payments (positive coefficient)**: Payment history predicts future defaults\n",
    "  - OR = 1.08: Each additional day late increases default odds by 8%\n",
    "  - **Action**: Flag accounts with increasing late payment trends\n",
    "\n",
    "**Model Deployment**:\n",
    "1. **Score new applicants**: Calculate probability of default\n",
    "2. **Risk-based pricing**: \n",
    "   - Low risk (P < 2%): Prime rate (15% APR)\n",
    "   - Medium risk (2% < P < 8%): Standard rate (22% APR)\n",
    "   - High risk (P > 8%): Decline or subprime rate (28% APR)\n",
    "3. **Credit limits**:\n",
    "   - Low risk: High credit limits\n",
    "   - High risk: Low credit limits or require secured card\n",
    "\n",
    "**E-Commerce Conversion Prediction**:\n",
    "```r\n",
    "conversion_model <- glm(purchased ~ time_on_site + pages_viewed + cart_value + is_returning_visitor,\n",
    "                        data = session_data,\n",
    "                        family = \"binomial\")\n",
    "```\n",
    "**Business Use**:\n",
    "- **Predict purchase probability in real-time**\n",
    "- If probability > 70%: No intervention needed\n",
    "- If probability 40-70%: Show discount offer to push over edge\n",
    "- If probability < 40%: Capture email for remarketing\n",
    "\n",
    "**Email Marketing Response**:\n",
    "```r\n",
    "response_model <- glm(opened ~ subject_line_type + send_time + customer_segment + past_open_rate,\n",
    "                      family = \"binomial\")\n",
    "```\n",
    "**Business Value**:\n",
    "- Identify subject lines with highest open probability\n",
    "- Optimize send times by customer segment\n",
    "- Focus campaigns on high-propensity customers (cost efficiency)\n",
    "\n",
    "**Employee Attrition**:\n",
    "```r\n",
    "attrition_model <- glm(left_company ~ satisfaction_score + years_since_promotion + \n",
    "                                       manager_rating + commute_distance,\n",
    "                       family = \"binomial\")\n",
    "```\n",
    "**Business Application**:\n",
    "- Score all employees monthly\n",
    "- High attrition risk (P > 30%): Manager intervention, retention bonus\n",
    "- Medium risk: Career development conversation\n",
    "- Low risk: Standard engagement activities\n",
    "\n",
    "### Model Evaluation Metrics\n",
    "\n",
    "**Accuracy Alone is Misleading**:\n",
    "- If 95% of customers don't default, model predicting \"no default\" for everyone is 95% accurate but useless\n",
    "\n",
    "**Better Metrics**:\n",
    "1. **Confusion Matrix**:\n",
    "   - True Positives: Correctly predicted defaults\n",
    "   - False Positives: False alarms (denied good customers)\n",
    "   - False Negatives: Missed defaults (costly!)\n",
    "   - True Negatives: Correctly predicted non-defaults\n",
    "\n",
    "2. **Precision & Recall**:\n",
    "   - Precision: Of predicted defaults, how many actually defaulted?\n",
    "   - Recall: Of actual defaults, how many did we predict?\n",
    "   - **Trade-off**: Stricter threshold → Higher precision, lower recall\n",
    "\n",
    "3. **ROC Curve & AUC**:\n",
    "   - Plots True Positive Rate vs. False Positive Rate\n",
    "   - AUC (Area Under Curve): Overall model discrimination\n",
    "   - AUC = 0.5: Random guessing\n",
    "   - AUC = 0.7-0.8: Acceptable\n",
    "   - AUC = 0.8-0.9: Excellent\n",
    "   - AUC > 0.9: Outstanding (or data leakage!)\n",
    "\n",
    "**Business Decision**:\n",
    "- Set threshold based on cost of false positives vs. false negatives\n",
    "- **Credit cards**: False negative (missed default) costs $5,000; false positive (denied good customer) costs $200 in lost profit\n",
    "- **Optimal threshold**: Minimize expected total cost, not maximize accuracy\n",
    "\n",
    "**Best Practice**:\n",
    "- Validate on out-of-time sample (future data), not just holdout set\n",
    "- Monitor model performance in production (calibration can drift)\n",
    "- Retrain periodically as customer behavior changes\n",
    "- Document decision thresholds and business justification\n",
    "- Ensure compliance with fair lending / discrimination regulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d852b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Import Data for Logit Model\n",
    "logit_data <- read.csv(\"/workspaces/MS3313_base_template/data/module_1/Session1-CC_Default_data_XXX_Bank.csv\", header=T)\n",
    "\n",
    "# View variable names\n",
    "names(logit_data)\n",
    "\n",
    "# Display first few rows\n",
    "head(logit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725181a6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fit Logistic Regression Model\n",
    "mylogit <- glm(d_dummy ~ student_dummy + balance + income + gender + edu + avg_late_payment, \n",
    "               data = logit_data, \n",
    "               family = \"binomial\")\n",
    "\n",
    "# Display model summary\n",
    "summary(mylogit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739214d",
   "metadata": {},
   "source": [
    "## Logistic Regression Diagnostics: Interpreting Model Performance\n",
    "\n",
    "### Technical Overview\n",
    "After fitting a logistic regression, several diagnostic measures assess model quality and provide business-relevant interpretations:\n",
    "\n",
    "**Key Metrics**:\n",
    "1. **Odds Ratios (OR)**: `exp(coefficients)`\n",
    "   - Multiplicative effect on odds of outcome\n",
    "   - OR > 1: Variable increases odds\n",
    "   - OR < 1: Variable decreases odds\n",
    "   \n",
    "2. **Confidence Intervals**: Uncertainty around odds ratios\n",
    "   - If 95% CI excludes 1.0 → Statistically significant effect\n",
    "   \n",
    "3. **McFadden's Pseudo-R²**: Model fit measure\n",
    "   - Range: 0 to 1 (unlike true R²)\n",
    "   - 0.2-0.4 considered excellent for logistic models\n",
    "   - **Not comparable to linear regression R²**\n",
    "\n",
    "4. **Predicted Probabilities**: Model's probability estimates for each case\n",
    "   \n",
    "5. **Deviance Residuals**: Errors in probability predictions\n",
    "\n",
    "### Business Importance\n",
    "**Diagnostics translate statistical output into actionable business insights**:\n",
    "\n",
    "1. **Odds Ratios = Business Impact**:\n",
    "   - \"Each $1000 in balance multiplies default odds by 1.005 (0.5% increase)\"\n",
    "   - Executives understand \"50% higher risk\" better than \"coefficient = 0.405\"\n",
    "   - **Prioritization**: Focus on variables with large odds ratios\n",
    "\n",
    "2. **Confidence Intervals = Decision Confidence**:\n",
    "   - Wide CI: Uncertain effect, need more data or better measurement\n",
    "   - Narrow CI: Reliable effect, safe to base decisions on\n",
    "   - **Risk Management**: Don't bet business on uncertain coefficients\n",
    "\n",
    "3. **Model Fit = Predictive Power**:\n",
    "   - McFadden's R² = 0.05: Model barely better than random\n",
    "   - McFadden's R² = 0.30: Model has strong predictive power\n",
    "   - **Business Value**: High R² justifies investment in data collection/modeling\n",
    "\n",
    "4. **Predicted Probabilities = Risk Scores**:\n",
    "   - Convert to risk tiers for operational use\n",
    "   - Enable cost-benefit analysis for each customer\n",
    "   - **Automation**: Rules engine uses probabilities for automated decisions\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Credit Risk Interpretation**:\n",
    "```r\n",
    "# Odds ratios\n",
    "exp(coef(credit_model))\n",
    "# student_dummy: OR = 0.6 → Students have 40% lower default odds (controlling for other factors)\n",
    "# balance: OR = 1.005 → Each $1 in balance increases default odds by 0.5%\n",
    "# income: OR = 0.99 → Each $1K income decreases default odds by 1%\n",
    "```\n",
    "\n",
    "**Business Decisions**:\n",
    "- **Student accounts**: Safer than non-students (contrary to intuition!) → Market to students\n",
    "- **Balance management**: High balances are risky → Set credit limits carefully\n",
    "- **Income verification**: Important but modest effect → Cost-benefit of verification process\n",
    "\n",
    "**Confidence Intervals for Business Planning**:\n",
    "```r\n",
    "exp(confint(credit_model))\n",
    "#                    2.5%     97.5%\n",
    "# student_dummy     0.45      0.80\n",
    "# balance          1.004     1.006\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Student effect: 95% confident OR is between 0.45-0.80 (definitely protective)\n",
    "- Balance effect: 95% confident OR is between 1.004-1.006 (narrow range, precise estimate)\n",
    "\n",
    "**Strategic Implication**:\n",
    "- Student effect is reliable → Build student-focused product line\n",
    "- Balance effect is precise → Use in automated credit limit algorithms\n",
    "\n",
    "**McFadden R² for Model Comparison**:\n",
    "```r\n",
    "library(pscl)\n",
    "pR2(credit_model)\n",
    "#   llh       llhNull          G2      McFadden          r2ML          r2CU \n",
    "# -1250.5   -1800.2       1099.4      0.305         0.250         0.380\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- McFadden R² = 0.305 (Excellent - model explains 30.5% of deviance)\n",
    "- Much better than null model (no predictors)\n",
    "\n",
    "**Business Value**:\n",
    "- Model is good enough for production use\n",
    "- Investing in additional variables may yield only marginal improvements\n",
    "- Focus resources on implementation, not endless model refinement\n",
    "\n",
    "**Predicted Probabilities for Customer Segmentation**:\n",
    "```r\n",
    "predicted_probs <- predict(credit_model, type = \"response\")\n",
    "summary(predicted_probs)\n",
    "#    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n",
    "# 0.0001   0.0250    0.0850   0.1200   0.1800   0.9500\n",
    "```\n",
    "\n",
    "**Business Application**:\n",
    "Create risk tiers:\n",
    "```r\n",
    "risk_tier <- cut(predicted_probs,\n",
    "                 breaks = c(0, 0.05, 0.15, 0.30, 1),\n",
    "                 labels = c(\"Excellent\", \"Good\", \"Fair\", \"Poor\"))\n",
    "\n",
    "table(risk_tier)\n",
    "# Excellent: 40% (approve at prime rate)\n",
    "# Good: 35% (approve at standard rate)\n",
    "# Fair: 20% (approve with higher rate or require collateral)\n",
    "# Poor: 5% (decline or manual underwriting)\n",
    "```\n",
    "\n",
    "**Revenue Impact**:\n",
    "- Approve top 75% (Excellent + Good) automatically → Fast decisions, low cost\n",
    "- Middle 20% (Fair) → Manual review (worth the cost for approval rate)\n",
    "- Bottom 5% (Poor) → Auto-decline (save underwriting costs)\n",
    "\n",
    "**Deviance Residuals for Quality Control**:\n",
    "```r\n",
    "dev_resid <- residuals(credit_model, type = \"deviance\")\n",
    "large_residuals <- which(abs(dev_resid) > 3)\n",
    "\n",
    "# Investigate cases with large residuals\n",
    "problem_cases <- data[large_residuals, ]\n",
    "```\n",
    "\n",
    "**Business Use**:\n",
    "- Large residuals = Model very wrong about these cases\n",
    "- Could indicate:\n",
    "  - Data errors (investigate and fix)\n",
    "  - Fraud (these cases behave unusually)\n",
    "  - Missing variables (consistent pattern in residuals suggests missing predictor)\n",
    "  \n",
    "**Example**: \n",
    "- Model predicts low default risk, but customer defaulted\n",
    "- Investigation reveals: Recent job loss not in data\n",
    "- **Action**: Add employment stability variable to model\n",
    "\n",
    "### Model Validation Workflow\n",
    "\n",
    "**Step 1: Check Coefficients Make Business Sense**\n",
    "- Income should decrease default risk (negative coefficient)\n",
    "- Late payment history should increase default risk (positive coefficient)\n",
    "- If results contradict logic → Data problem or specification error\n",
    "\n",
    "**Step 2: Examine Odds Ratios and CIs**\n",
    "- Focus on largest ORs (biggest business impact)\n",
    "- Verify CIs are reasonable (exclude 1.0 for significant effects)\n",
    "\n",
    "**Step 3: Assess Overall Fit**\n",
    "- McFadden R² > 0.2: Good model\n",
    "- Compare to baseline (null model, industry benchmarks)\n",
    "\n",
    "**Step 4: Validate on Holdout Data**\n",
    "- Refit model on training data\n",
    "- Calculate metrics on test data\n",
    "- Similar performance → Model will generalize\n",
    "\n",
    "**Step 5: Business Case Analysis**\n",
    "- Calculate expected profit/loss at different decision thresholds\n",
    "- **Example**: At 5% default threshold:\n",
    "  - 80% approval rate, 8% portfolio default rate\n",
    "  - Expected profit = $X million\n",
    "- **Optimize**: Find threshold maximizing expected profit\n",
    "\n",
    "**Best Practice**:\n",
    "- Present odds ratios (not raw coefficients) to stakeholders\n",
    "- Use confidence intervals to communicate uncertainty\n",
    "- Validate that high-risk scores actually correspond to high default rates\n",
    "- Monitor model performance over time (yearly recalibration typical for credit models)\n",
    "- Document all business rules derived from model (threshold selections, risk tiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506cf08",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Odds ratios (exponentiated coefficients)\n",
    "exp(coef(mylogit))\n",
    "\n",
    "# Odds ratios with 95% Confidence Intervals\n",
    "exp(cbind(OR = coef(mylogit), confint(mylogit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae398e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# McFadden R-squared (requires library pscl)\n",
    "pR2(mylogit)\n",
    "\n",
    "# 95% CI for the coefficients\n",
    "confint(mylogit)\n",
    "\n",
    "# 95% CI for exponentiated coefficients\n",
    "exp(confint(mylogit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c92db",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Predicted probabilities\n",
    "predicted_probs <- predict(mylogit, type=\"response\")\n",
    "head(predicted_probs)\n",
    "\n",
    "# Deviance residuals\n",
    "dev_residuals <- residuals(mylogit, type=\"deviance\")\n",
    "head(dev_residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb552a24",
   "metadata": {},
   "source": [
    "## Train-Test Split: Foundation of Model Validation\n",
    "\n",
    "### Technical Overview\n",
    "**Train-test split** divides data into two independent sets:\n",
    "\n",
    "**Training Set (typically 70-80%)**:\n",
    "- Used to fit/estimate model parameters\n",
    "- Model \"sees\" this data during learning\n",
    "\n",
    "**Test Set (typically 20-30%)**:\n",
    "- Used to evaluate model performance\n",
    "- Model has never \"seen\" this data\n",
    "- Simulates performance on future, unseen data\n",
    "\n",
    "```r\n",
    "set.seed(100)  # Reproducibility\n",
    "train_indices <- sample(1:nrow(data), 0.8 * nrow(data))\n",
    "train_data <- data[train_indices, ]\n",
    "test_data <- data[-train_indices, ]\n",
    "```\n",
    "\n",
    "**Critical**: Use `set.seed()` for reproducibility - results should be same every time code runs\n",
    "\n",
    "### Business Importance\n",
    "**Validation on unseen data prevents costly deployment of overfitted models**:\n",
    "\n",
    "1. **Overfitting: The $Million Dollar Mistake**:\n",
    "   - **Problem**: Model fits training data perfectly but fails on new data\n",
    "   - **Example**: Demand forecast accurate on historical data, but terrible for next quarter\n",
    "   - **Cost**: Excess inventory ($500K), stockouts (lost sales $300K), reputation damage\n",
    "   - **Prevention**: Test set reveals overfitting BEFORE production deployment\n",
    "\n",
    "2. **Realistic Performance Expectations**:\n",
    "   - Training accuracy: 95% (overly optimistic)\n",
    "   - Test accuracy: 87% (realistic expectation)\n",
    "   - **Business Value**: Set correct expectations with stakeholders\n",
    "   - Avoid over-promising (\"We'll reduce churn by 25%\") based on training performance\n",
    "\n",
    "3. **Model Selection**:\n",
    "   - Complex model: 90% training, 82% test → Overfitting\n",
    "   - Simple model: 85% training, 84% test → Better generalization\n",
    "   - **Choose simpler model** - test performance is what matters in production\n",
    "\n",
    "4. **Budget Justification**:\n",
    "   - \"Model predicts customer churn with 78% accuracy on test set\"\n",
    "   - CFO understands 78% of prevented churn → ROI calculation\n",
    "   - Training accuracy meaningless for business case\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Demand Forecasting**:\n",
    "```r\n",
    "# Training: Historical sales Jan 2020 - Dec 2023\n",
    "# Test: Sales Jan 2024 - Mar 2024\n",
    "\n",
    "model <- lm(sales ~ price + promotions + seasonality, data = train_data)\n",
    "predictions <- predict(model, test_data)\n",
    "\n",
    "# Compare predictions to actuals\n",
    "mae <- mean(abs(predictions - test_data$sales))\n",
    "```\n",
    "\n",
    "**Business Decision**:\n",
    "- MAE on test set = $50K per month\n",
    "- Annual forecast error = $600K\n",
    "- **Implication**: Maintain safety stock to buffer forecast errors\n",
    "- **Alternatives**: \n",
    "  - Invest in better data (weather, competitor prices) to improve model\n",
    "  - Accept error and plan for buffer inventory costs\n",
    "\n",
    "**Credit Scoring Model Deployment**:\n",
    "```r\n",
    "# Training: Loan applications from 2020-2022\n",
    "# Test: Loan applications from 2023\n",
    "\n",
    "credit_model <- glm(default ~ balance + income + credit_history, \n",
    "                    data = train_data, \n",
    "                    family = \"binomial\")\n",
    "\n",
    "# Evaluate on 2023 data\n",
    "test_predictions <- predict(credit_model, test_data, type = \"response\")\n",
    "```\n",
    "\n",
    "**Business Validation**:\n",
    "- **Training AUC**: 0.85 (looks great!)\n",
    "- **Test AUC**: 0.78 (still good, but more realistic)\n",
    "- **Decision**: Deploy model, but expect 78% discrimination, not 85%\n",
    "- **Financial Planning**: \n",
    "  - Expected default rate at 5% threshold: 8% (based on test set)\n",
    "  - Budget for 8% losses, not the 6% training set suggested\n",
    "\n",
    "**Marketing Response Model**:\n",
    "```r\n",
    "# Training: Email campaigns Jan-Sep 2024\n",
    "# Test: Email campaigns Oct-Dec 2024\n",
    "\n",
    "response_model <- glm(responded ~ customer_segment + email_subject + send_time,\n",
    "                      data = train_data,\n",
    "                      family = \"binomial\")\n",
    "```\n",
    "\n",
    "**ROI Calculation**:\n",
    "```r\n",
    "# Test set performance\n",
    "test_pred <- predict(response_model, test_data, type = \"response\")\n",
    "\n",
    "# Target top 30% (highest predicted response probability)\n",
    "top_30_pct <- test_data[test_pred > quantile(test_pred, 0.7), ]\n",
    "actual_response_rate <- mean(top_30_pct$responded)\n",
    "# Result: 15% response rate in test set\n",
    "\n",
    "# Business case\n",
    "email_cost_per_contact <- 0.10\n",
    "revenue_per_response <- 50\n",
    "list_size <- 100000\n",
    "\n",
    "# If we email top 30% (30,000 customers)\n",
    "costs <- 30000 * 0.10  # $3,000\n",
    "expected_responses <- 30000 * 0.15  # 4,500\n",
    "revenue <- 4500 * 50  # $225,000\n",
    "profit <- revenue - costs  # $222,000\n",
    "\n",
    "# Decision: Deploy model, target high-probability customers\n",
    "```\n",
    "\n",
    "### Statistical Best Practices\n",
    "\n",
    "**Random Sampling**:\n",
    "```r\n",
    "set.seed(123)\n",
    "train_indices <- sample(1:nrow(data), 0.8 * nrow(data))\n",
    "```\n",
    "- Ensures training and test sets have similar distributions\n",
    "- Prevents bias (e.g., all high-value customers in training set)\n",
    "\n",
    "**Stratified Sampling** (for imbalanced outcomes):\n",
    "```r\n",
    "library(caret)\n",
    "train_indices <- createDataPartition(data$outcome, p = 0.8, list = FALSE)\n",
    "```\n",
    "- Ensures rare events (defaults, churns) appear in both sets\n",
    "- Critical when outcome is rare (<10% of data)\n",
    "\n",
    "**Time Series Consideration**:\n",
    "- **Don't use random split for time series!**\n",
    "- Use chronological split: Training = past, Test = future\n",
    "- Simulates real forecasting scenario\n",
    "\n",
    "```r\n",
    "# Time series split\n",
    "cutoff_date <- as.Date(\"2024-01-01\")\n",
    "train_data <- data[data$date < cutoff_date, ]\n",
    "test_data <- data[data$date >= cutoff_date, ]\n",
    "```\n",
    "\n",
    "**Common Pitfalls**:\n",
    "1. **Data Leakage**: Test data information bleeds into training\n",
    "   - Example: Scaling using full dataset statistics before split\n",
    "   - **Fix**: Fit scaler on training data only, apply to test data\n",
    "\n",
    "2. **Small Test Sets**: \n",
    "   - Test set too small → Unreliable performance estimates\n",
    "   - **Rule**: Minimum 100 cases in test set for reliable metrics\n",
    "\n",
    "3. **Not Setting Seed**: \n",
    "   - Results change every time → Can't reproduce analysis\n",
    "   - **Fix**: Always use `set.seed()`\n",
    "\n",
    "### Cross-Validation Alternative\n",
    "\n",
    "**When to use k-fold cross-validation instead**:\n",
    "- Small datasets (< 1000 observations)\n",
    "- Want more robust performance estimate\n",
    "- Willing to train k models (computationally expensive)\n",
    "\n",
    "```r\n",
    "library(caret)\n",
    "cv_control <- trainControl(method = \"cv\", number = 10)\n",
    "cv_model <- train(outcome ~ predictors, data = data, method = \"lm\", trControl = cv_control)\n",
    "```\n",
    "\n",
    "**Best Practice**:\n",
    "- **Standard datasets (>1000 cases)**: Train-test split (80/20)\n",
    "- **Small datasets (<1000 cases)**: 10-fold cross-validation\n",
    "- **Time series**: Chronological split or rolling-window validation\n",
    "- **Always report test/validation performance**, not training performance\n",
    "- Keep test set locked until final evaluation (no peeking!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2efe8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load salary and performance data\n",
    "salarydata <- read.csv(\"/workspaces/MS3313_base_template/data/module_1/Session1-Salary and performance AZ.csv\", header=T)\n",
    "\n",
    "# View variable names and data structure\n",
    "names(salarydata)\n",
    "str(salarydata)\n",
    "head(salarydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0f2bf",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create Training and Test data\n",
    "set.seed(100)  # Setting seed to reproduce results of random sampling\n",
    "\n",
    "# Generate row indices for training data (80% of data)\n",
    "trainingRowIndex <- sample(1:nrow(salarydata), 0.8*nrow(salarydata))\n",
    "\n",
    "# Split into training and test datasets\n",
    "trainingData <- salarydata[trainingRowIndex, ]  # Model training data\n",
    "testData <- salarydata[-trainingRowIndex, ]      # Test data\n",
    "\n",
    "# Verify the split\n",
    "cat(\"Training data rows:\", nrow(trainingData), \"\\n\")\n",
    "cat(\"Test data rows:\", nrow(testData), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65918892",
   "metadata": {},
   "source": [
    "## Model Training and Prediction: From Data to Decisions\n",
    "\n",
    "### Technical Overview\n",
    "After splitting data, the modeling workflow is:\n",
    "\n",
    "**Step 1: Train Model on Training Data**\n",
    "```r\n",
    "model <- lm(y ~ x1 + x2 + x3, data = training_data)\n",
    "```\n",
    "- Model learns relationships from training set only\n",
    "- Coefficients estimated to minimize training set errors\n",
    "\n",
    "**Step 2: Generate Predictions on Test Data**\n",
    "```r\n",
    "predictions <- predict(model, test_data)\n",
    "```\n",
    "- Apply trained model to unseen test data\n",
    "- Simulates real-world deployment scenario\n",
    "\n",
    "**Step 3: Evaluate Performance**\n",
    "```r\n",
    "summary(model)  # Model statistics\n",
    "AIC(model)      # Model selection criterion\n",
    "```\n",
    "\n",
    "### Business Importance\n",
    "**This workflow ensures models perform well in production, not just on historical data**:\n",
    "\n",
    "1. **Simulates Real Deployment**:\n",
    "   - Training on past data → Predicting future outcomes\n",
    "   - Test set = \"future data the model hasn't seen\"\n",
    "   - Performance on test set = expected production performance\n",
    "   - **Business Value**: Reliable ROI estimates for model deployment\n",
    "\n",
    "2. **Model Diagnostics Guide Improvements**:\n",
    "   - **AIC (Akaike Information Criterion)**: Lower = better model\n",
    "   - Compare models: Simple vs. complex, different variable sets\n",
    "   - **Example**: \n",
    "     - Model A (5 variables): AIC = 1250\n",
    "     - Model B (10 variables): AIC = 1248\n",
    "     - Model A preferred (simpler, nearly same fit)\n",
    "\n",
    "3. **Documentation for Governance**:\n",
    "   - `summary()` output provides audit trail\n",
    "   - Coefficients, p-values, R² documented\n",
    "   - Regulatory compliance (fair lending, insurance)\n",
    "   - **Business Value**: Defensible models for audits/litigation\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Sales Forecasting Model**:\n",
    "```r\n",
    "# Train on historical data\n",
    "sales_model <- lm(monthly_sales ~ marketing_spend + price + seasonality + competitor_price,\n",
    "                  data = train_data)\n",
    "\n",
    "# Predict next quarter\n",
    "q4_predictions <- predict(sales_model, test_data)\n",
    "\n",
    "# Evaluate model diagnostics\n",
    "summary(sales_model)\n",
    "# R² = 0.78 → Model explains 78% of sales variation\n",
    "# marketing_spend coefficient = 2.5 (p < 0.001) → $1 spend → $2.50 sales\n",
    "# price coefficient = -500 (p < 0.001) → $1 price increase → 500 unit decrease\n",
    "\n",
    "AIC(sales_model)  # Use to compare against alternative specifications\n",
    "```\n",
    "\n",
    "**Business Decisions**:\n",
    "- **R² = 0.78**: Strong model → Trust forecasts for inventory planning\n",
    "- **Marketing ROI = 2.5**: Every dollar spent generates $2.50 → Increase budget\n",
    "- **Price Sensitivity = -500**: High elasticity → Avoid price increases\n",
    "- **Q4 Forecast**: Predicted sales = 12,500 units ± margin of error\n",
    "  - Order inventory: 13,000 units (forecast + safety stock)\n",
    "\n",
    "**Employee Performance Prediction**:\n",
    "```r\n",
    "performance_model <- lm(annual_sales ~ experience_years + training_hours + manager_rating,\n",
    "                        data = train_data)\n",
    "\n",
    "# Predict performance for new hires (test set = recent hires)\n",
    "new_hire_predictions <- predict(performance_model, test_data)\n",
    "\n",
    "summary(performance_model)\n",
    "# experience_years: +$15K sales per year (p < 0.01)\n",
    "# training_hours: +$800 sales per hour (p < 0.05)\n",
    "# manager_rating: +$5K sales per rating point (p = 0.20, not significant)\n",
    "```\n",
    "\n",
    "**Business Insights**:\n",
    "- **Experience**: Hire experienced salespeople when possible (clear ROI)\n",
    "- **Training**: $800 return per hour → 40-hour training = $32K sales increase\n",
    "  - Training cost: $5K → ROI = 540%\n",
    "  - **Decision**: Invest heavily in training programs\n",
    "- **Manager Rating**: Not significant → Review performance evaluation process\n",
    "\n",
    "**Customer Lifetime Value Prediction**:\n",
    "```r\n",
    "ltv_model <- lm(lifetime_value ~ first_purchase_amount + days_to_second_purchase + \n",
    "                                 acquisition_channel + customer_segment,\n",
    "                data = train_data)\n",
    "\n",
    "# Score new customers acquired this month\n",
    "new_customer_ltv <- predict(ltv_model, new_customers)\n",
    "\n",
    "# Segment by predicted LTV\n",
    "high_value <- new_customers[new_customer_ltv > 1000, ]\n",
    "medium_value <- new_customers[new_customer_ltv > 500 & new_customer_ltv <= 1000, ]\n",
    "low_value <- new_customers[new_customer_ltv <= 500, ]\n",
    "```\n",
    "\n",
    "**Business Application**:\n",
    "- **High Value Customers (LTV > $1000)**:\n",
    "  - Assign dedicated account manager\n",
    "  - Priority customer service\n",
    "  - Exclusive offers/early access\n",
    "  - **Investment**: $100 per customer → ROI based on $1000 LTV\n",
    "\n",
    "- **Medium Value Customers ($500-$1000)**:\n",
    "  - Standard email nurturing\n",
    "  - Periodic promotions\n",
    "  - **Investment**: $20 per customer\n",
    "\n",
    "- **Low Value Customers (<$500)**:\n",
    "  - Automated marketing only\n",
    "  - Minimal investment\n",
    "  - **Goal**: Maximize profit margin, not lifetime value\n",
    "\n",
    "### Model Comparison Example\n",
    "\n",
    "**Scenario**: Should we include additional variables (costs more to collect)?\n",
    "\n",
    "```r\n",
    "# Simple model (existing data)\n",
    "model_simple <- lm(sales ~ price + advertising, data = train_data)\n",
    "AIC(model_simple)  # 1850\n",
    "\n",
    "# Complex model (requires new data collection)\n",
    "model_complex <- lm(sales ~ price + advertising + customer_sentiment + \n",
    "                            competitor_promotions + economic_indicators,\n",
    "                    data = train_data)\n",
    "AIC(model_complex)  # 1820\n",
    "```\n",
    "\n",
    "**Business Decision**:\n",
    "- AIC improvement: 30 points (moderate improvement)\n",
    "- Cost of additional data: \n",
    "  - Customer sentiment: $50K/year (survey vendor)\n",
    "  - Competitor promotions: $30K/year (market research)\n",
    "  - Economic indicators: Free (public data)\n",
    "  \n",
    "- **Analysis**:\n",
    "  - Better predictions → Improved inventory planning → $200K/year savings\n",
    "  - Data costs: $80K/year\n",
    "  - **Net benefit**: $120K/year\n",
    "  - **Decision**: Invest in complex model (positive ROI)\n",
    "\n",
    "### Interpreting Model Output\n",
    "\n",
    "**Key Statistics from `summary()`**:\n",
    "\n",
    "1. **Coefficients**: \n",
    "   - Magnitude: Business impact of each variable\n",
    "   - Sign: Direction of relationship\n",
    "   \n",
    "2. **Standard Errors**: \n",
    "   - Uncertainty in coefficient estimates\n",
    "   - Large SE → Unreliable coefficient\n",
    "\n",
    "3. **P-values**: \n",
    "   - Statistical significance (typically < 0.05 threshold)\n",
    "   - **Business context matters**: Small effect can be significant but not meaningful\n",
    "\n",
    "4. **R²**: \n",
    "   - Proportion of variance explained\n",
    "   - R² = 0.90: Excellent predictive model\n",
    "   - R² = 0.30: Weak predictive model (but coefficients may still be insightful)\n",
    "\n",
    "5. **F-statistic**: \n",
    "   - Overall model significance\n",
    "   - Low p-value: Model is better than intercept-only\n",
    "\n",
    "**Best Practice**:\n",
    "- **Always predict on test set, not training set**\n",
    "- Compare multiple models using AIC/BIC\n",
    "- Interpret coefficients in business context (not just \"statistically significant\")\n",
    "- Document model assumptions and limitations\n",
    "- Plan for model refresh (retrain quarterly/annually as data changes)\n",
    "- Set up monitoring: Track test set performance over time in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50ab77",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Build the model on training data\n",
    "lmMod <- lm(Performance_Indicator ~ Salary_Hike_2022, data=trainingData)\n",
    "\n",
    "# Generate predictions on test data\n",
    "Performance_IndicatorPred <- predict(lmMod, testData)\n",
    "\n",
    "# Display model summary and diagnostics\n",
    "summary(lmMod)\n",
    "AIC(lmMod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276dcf4b",
   "metadata": {},
   "source": [
    "## Model Accuracy Metrics: Measuring Prediction Quality\n",
    "\n",
    "### Technical Overview\n",
    "After generating predictions, multiple metrics assess how well the model performs:\n",
    "\n",
    "**1. Correlation Accuracy**:\n",
    "```r\n",
    "cor(actuals, predictions)\n",
    "```\n",
    "- Range: -1 to +1\n",
    "- Measures linear relationship strength\n",
    "- +1 = perfect positive correlation\n",
    "- **Limitation**: Doesn't penalize systematic over/under-prediction\n",
    "\n",
    "**2. Min-Max Accuracy**:\n",
    "```r\n",
    "mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))\n",
    "```\n",
    "- Range: 0 to 1 (higher is better)\n",
    "- Ratio of smaller to larger value (actual vs. predicted)\n",
    "- **Advantage**: Scale-invariant, works for any units\n",
    "\n",
    "**3. MAPE (Mean Absolute Percentage Error)**:\n",
    "```r\n",
    "mean(abs((predictions - actuals) / actuals))\n",
    "```\n",
    "- Average absolute error as percentage of actual\n",
    "- Interpretable: \"Model is off by X% on average\"\n",
    "- **Limitation**: Undefined for actuals = 0, biased toward underforecasts\n",
    "\n",
    "**4. Alternative Metrics** (not shown but commonly used):\n",
    "- **MAE (Mean Absolute Error)**: Average absolute difference\n",
    "- **RMSE (Root Mean Squared Error)**: Penalizes large errors more\n",
    "- **R²**: Proportion of variance explained (from model summary)\n",
    "\n",
    "### Business Importance\n",
    "**Different metrics reveal different aspects of model quality - choose based on business priorities**:\n",
    "\n",
    "1. **Correlation**: Captures Trend Accuracy\n",
    "   - Correlation = 0.95: Model captures 95% of variation pattern\n",
    "   - **Use Case**: Strategic planning (directional trends matter)\n",
    "   - **Example**: Stock market forecasting (direction > exact price)\n",
    "\n",
    "2. **Min-Max Accuracy**: Fair Comparison Across Scales\n",
    "   - Works for sales ($100-$1M range) and units (10-10,000 range)\n",
    "   - **Use Case**: Comparing forecast accuracy across product lines\n",
    "   - **Example**: Is model more accurate for high-volume or high-value products?\n",
    "\n",
    "3. **MAPE**: Intuitive Business Communication\n",
    "   - \"Forecast is accurate within 15%\" is clear to executives\n",
    "   - Directly relates to business tolerances\n",
    "   - **Use Case**: Operational planning (inventory, staffing)\n",
    "   - **Example**: 5% MAPE acceptable for perishable goods ordering\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Demand Forecasting Evaluation**:\n",
    "```r\n",
    "# Predicted: 1050 units, Actual: 1000 units\n",
    "# Error: 50 units (5% MAPE)\n",
    "\n",
    "business_impact <- data.frame(\n",
    "  forecast = c(950, 1000, 1050, 1100),\n",
    "  actual = 1000,\n",
    "  scenario = c(\"Underforecast\", \"Perfect\", \"Slight Over\", \"Overforecast\")\n",
    ")\n",
    "\n",
    "# Cost analysis\n",
    "holding_cost_per_unit <- 5      # Excess inventory\n",
    "stockout_cost_per_unit <- 50    # Lost sales + expedited shipping\n",
    "\n",
    "# Calculate costs\n",
    "business_impact$cost <- ifelse(\n",
    "  business_impact$forecast < business_impact$actual,\n",
    "  (business_impact$actual - business_impact$forecast) * stockout_cost_per_unit,  # Underforecast\n",
    "  (business_impact$forecast - business_impact$actual) * holding_cost_per_unit    # Overforecast\n",
    ")\n",
    "\n",
    "# Result:\n",
    "# Underforecast 50 units: $2,500 cost (stockouts)\n",
    "# Overforecast 50 units: $250 cost (holding)\n",
    "# Perfect: $0 cost\n",
    "```\n",
    "\n",
    "**Business Insight**: \n",
    "- MAPE treats over/under errors equally (5% error either way)\n",
    "- **But business costs are asymmetric!**\n",
    "- Stockouts cost 10x holding costs\n",
    "- **Implication**: Bias forecasts slightly high to avoid costly stockouts\n",
    "\n",
    "**Sales Performance Prediction**:\n",
    "```r\n",
    "# Salesperson performance model\n",
    "actuals <- c(100, 150, 200, 250, 120)  # Actual sales ($K)\n",
    "predicted <- c(110, 140, 210, 260, 115) # Predicted sales ($K)\n",
    "\n",
    "# Metrics\n",
    "correlation <- cor(actuals, predicted)  # 0.99 (excellent trend capture)\n",
    "mape <- mean(abs((predicted - actuals) / actuals))  # 0.06 (6% average error)\n",
    "\n",
    "# Business interpretation\n",
    "cat(\"Correlation:\", correlation, \"\\n\")\n",
    "cat(\"MAPE:\", mape * 100, \"%\\n\")\n",
    "```\n",
    "\n",
    "**Business Decision**:\n",
    "- **High correlation (0.99)**: Model correctly ranks salespeople (useful for incentives)\n",
    "- **MAPE 6%**: Territory assignments and quota setting reliable\n",
    "- **Action**: Use predictions to set individual quarterly targets\n",
    "  - Person A predicted $110K → Set quota at $100K (stretch goal)\n",
    "  - Person D predicted $260K → Set quota at $235K\n",
    "\n",
    "**Revenue Forecasting for Budgeting**:\n",
    "```r\n",
    "quarterly_forecast <- data.frame(\n",
    "  actual_revenue = c(10.2, 9.8, 11.5, 12.1),  # $M\n",
    "  predicted_revenue = c(10.5, 9.5, 11.2, 12.5)\n",
    ")\n",
    "\n",
    "mae <- mean(abs(quarterly_forecast$predicted_revenue - quarterly_forecast$actual_revenue))\n",
    "mape <- mean(abs((quarterly_forecast$predicted_revenue - quarterly_forecast$actual_revenue) / \n",
    "                 quarterly_forecast$actual_revenue))\n",
    "\n",
    "cat(\"MAE:\", mae, \"million\\n\")      # $0.3M average error\n",
    "cat(\"MAPE:\", mape * 100, \"%\\n\")     # 3% average error\n",
    "```\n",
    "\n",
    "**Budget Planning**:\n",
    "- Annual revenue forecast: $43M\n",
    "- MAPE: 3% → Confidence interval: $41.7M - $44.3M\n",
    "- **Conservative budget**: Use lower bound ($41.7M) for expense planning\n",
    "- **Stretch target**: Use point estimate ($43M) for revenue goals\n",
    "- **Contingency planning**: Prepare for $2.6M variance\n",
    "\n",
    "### Choosing the Right Metric\n",
    "\n",
    "**Scenario-Based Selection**:\n",
    "\n",
    "1. **Inventory Management** (costs asymmetric):\n",
    "   - **Avoid MAPE** (treats over/under equally)\n",
    "   - **Use custom cost function**: `sum(cost_of_error)`\n",
    "   - **Example**: Stockout costs >> Holding costs → Accept overforecasts\n",
    "\n",
    "2. **Financial Reporting** (absolute accuracy):\n",
    "   - **Use MAE or RMSE** (dollar terms, not percentages)\n",
    "   - **Example**: \"Forecast error ±$500K\" for CFO budgeting\n",
    "\n",
    "3. **Operational Efficiency** (percentage accuracy):\n",
    "   - **Use MAPE** (easy communication with operations)\n",
    "   - **Example**: \"Staffing model accurate within 10%\"\n",
    "\n",
    "4. **Product Comparison** (different scales):\n",
    "   - **Use Min-Max Accuracy or MAPE** (scale-invariant)\n",
    "   - **Example**: Compare forecast accuracy for $10 items vs. $1000 items\n",
    "\n",
    "5. **Strategic Direction** (trends matter more than exact values):\n",
    "   - **Use Correlation** (captures directional accuracy)\n",
    "   - **Example**: Market share forecasts for long-term planning\n",
    "\n",
    "### Business Tolerances\n",
    "\n",
    "**Industry Benchmarks**:\n",
    "- **Retail Demand Forecasting**: 15-25% MAPE typical\n",
    "- **Financial Forecasting**: 5-10% MAPE expected\n",
    "- **Short-term Sales**: <10% MAPE achievable\n",
    "- **Long-term Forecasts (>1 year)**: 20-30% MAPE common\n",
    "\n",
    "**Setting Acceptance Criteria**:\n",
    "```r\n",
    "# Define acceptable performance\n",
    "if (mape < 0.10) {\n",
    "  decision <- \"Deploy model - Excellent accuracy\"\n",
    "  confidence <- \"High confidence in operational planning\"\n",
    "} else if (mape < 0.20) {\n",
    "  decision <- \"Deploy with caution - Monitor performance\"\n",
    "  confidence <- \"Use forecasts with safety margins\"\n",
    "} else {\n",
    "  decision <- \"Improve model - Not ready for deployment\"\n",
    "  confidence <- \"Investigate data quality, add features, or change approach\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Combining Multiple Metrics\n",
    "\n",
    "**Comprehensive Evaluation**:\n",
    "```r\n",
    "# Calculate all metrics\n",
    "performance_summary <- data.frame(\n",
    "  metric = c(\"Correlation\", \"R²\", \"MAE\", \"MAPE\", \"Min-Max Accuracy\"),\n",
    "  value = c(\n",
    "    cor(actuals, predictions),\n",
    "    summary(model)$r.squared,\n",
    "    mean(abs(predictions - actuals)),\n",
    "    mean(abs((predictions - actuals) / actuals)),\n",
    "    mean(apply(cbind(actuals, predictions), 1, min) / \n",
    "         apply(cbind(actuals, predictions), 1, max))\n",
    "  ),\n",
    "  interpretation = c(\n",
    "    \"Trend accuracy\",\n",
    "    \"Variance explained\",\n",
    "    \"Average $ error\",\n",
    "    \"Average % error\",\n",
    "    \"Relative accuracy\"\n",
    "  )\n",
    ")\n",
    "\n",
    "print(performance_summary)\n",
    "```\n",
    "\n",
    "**Business Report**:\n",
    "- **Correlation: 0.92** → Model captures sales trends well\n",
    "- **R²: 0.85** → Explains 85% of variance (strong model)\n",
    "- **MAE: $45K** → Average error is $45K per prediction\n",
    "- **MAPE: 8%** → Typically within 8% of actual\n",
    "- **Min-Max: 0.93** → Predictions and actuals closely aligned\n",
    "\n",
    "**Executive Summary**:\n",
    "\"The model demonstrates strong predictive performance with 8% average error and 92% correlation. Recommended for production deployment to support quarterly planning.\"\n",
    "\n",
    "**Best Practice**:\n",
    "- **Report multiple metrics** (single metric can be misleading)\n",
    "- **Compare to baseline** (naïve forecast: \"next quarter = this quarter\")\n",
    "- **Set business-relevant thresholds** (not just statistical significance)\n",
    "- **Monitor metrics over time** (model performance degrades → retrain)\n",
    "- **Translate metrics to business impact** (8% MAPE → $X cost in inventory errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd789d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create actuals vs predictions dataframe\n",
    "actuals_preds <- data.frame(cbind(actuals=testData$Performance_Indicator, \n",
    "                                   predicteds=Performance_IndicatorPred))\n",
    "\n",
    "# Display first few rows\n",
    "head(actuals_preds)\n",
    "\n",
    "# Calculate correlation accuracy\n",
    "correlation_accuracy <- cor(actuals_preds)\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd650a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Min-Max accuracy (higher is better)\n",
    "min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))\n",
    "cat(\"Min-Max Accuracy:\", min_max_accuracy, \"\\n\")\n",
    "\n",
    "# Calculate MAPE - Mean Absolute Percentage Error (lower is better)\n",
    "mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals)) / actuals_preds$actuals)\n",
    "cat(\"MAPE:\", mape, \"\\n\")\n",
    "\n",
    "# Summary\n",
    "cat(\"\\n--- Model Performance Summary ---\\n\")\n",
    "cat(\"Min-Max Accuracy:\", round(min_max_accuracy, 4), \"(Higher is better)\\n\")\n",
    "cat(\"MAPE:\", round(mape, 4), \"(Lower is better)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b47a25",
   "metadata": {},
   "source": [
    "## 12. Exercise 1: Zadas Media Advertising Analysis\n",
    "\n",
    "**Business Context:**\n",
    "Zadas has been relying on media advertising for their business. Over the years, they have invested heavily in various media channels including radio, TV, newspaper, billboard, bus, and taxi advertising. They have data for sales and investment in various media for 7000 days (observed daily).\n",
    "\n",
    "**Key Challenges:**\n",
    "- Two or more media channels are often active simultaneously, creating multicollinearity issues\n",
    "- Difficult to understand the true impact of each media channel\n",
    "\n",
    "**Objectives:**\n",
    "1. Determine the effectiveness of various media in influencing sales\n",
    "2. Assess the relative impact and recommend resource reallocation\n",
    "3. Identify tipping points across various media channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c969d",
   "metadata": {},
   "source": [
    "### Step 1: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71d05a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load Zadas media advertising data\n",
    "zadas_data <- read.csv(\"/workspaces/MS3313_base_template/data/module_1/Session1-Excercise-Zadas Data.csv\", header=T)\n",
    "\n",
    "# View variable names\n",
    "names(zadas_data)\n",
    "\n",
    "# Display data structure\n",
    "str(zadas_data)\n",
    "\n",
    "# Display first few rows\n",
    "head(zadas_data)\n",
    "\n",
    "# Summary statistics\n",
    "summary(zadas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b25e6b5",
   "metadata": {},
   "source": [
    "### Step 2: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac5280",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for all numeric variables\n",
    "# This helps identify multicollinearity between media channels\n",
    "cor_matrix <- cor(zadas_data[sapply(zadas_data, is.numeric)])\n",
    "print(round(cor_matrix, 3))\n",
    "\n",
    "# Visualize correlation matrix with better label formatting\n",
    "library(ggplot2)\n",
    "library(reshape2)\n",
    "melted_cor <- melt(cor_matrix)\n",
    "ggplot(data = melted_cor, aes(x=Var1, y=Var2, fill=value)) + \n",
    "  geom_tile(color = \"white\") +\n",
    "  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n",
    "                       midpoint = 0, limit = c(-1,1)) +\n",
    "  theme_minimal() +\n",
    "  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10),\n",
    "\n",
    "        axis.text.y = element_text(size = 10),  coord_fixed()\n",
    "\n",
    "        plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),       x = \"\", y = \"\", fill = \"Correlation\") +\n",
    "\n",
    "        plot.margin = margin(10, 10, 10, 10)) +  labs(title = \"Correlation Matrix of Media Channels and Sales\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df7c5e",
   "metadata": {},
   "source": [
    "### Step 3: Full Linear Regression Model\n",
    "\n",
    "Fit a comprehensive model with all media channels to assess their individual effects on sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af399821",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fit full linear model with all media channels\n",
    "# Adjust column names based on your actual data\n",
    "# Example assuming columns: Sales, Radio, TV, Newspaper, Board, Bus, Taxi\n",
    "full_model <- lm(Sales ~ ., data=zadas_data)\n",
    "\n",
    "# Display model summary\n",
    "summary(full_model)\n",
    "\n",
    "# Check for multicollinearity using VIF (Variance Inflation Factor)\n",
    "# Install car package if needed: install.packages(\"car\")\n",
    "library(car)\n",
    "vif_values <- vif(full_model)\n",
    "print(\"Variance Inflation Factors:\")\n",
    "print(vif_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa91e9b",
   "metadata": {},
   "source": [
    "### Step 4: Stepwise Regression for Variable Selection\n",
    "\n",
    "Use stepwise regression to identify the most important media channels and address multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0ed7b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Perform stepwise regression\n",
    "step_model <- stepAIC(full_model, direction=\"both\")\n",
    "\n",
    "# Display the results\n",
    "step_model$anova\n",
    "\n",
    "# Summary of the final selected model\n",
    "summary(step_model)\n",
    "\n",
    "# Compare AIC values\n",
    "cat(\"\\nFull Model AIC:\", AIC(full_model), \"\\n\")\n",
    "cat(\"Stepwise Model AIC:\", AIC(step_model), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425ee49",
   "metadata": {},
   "source": [
    "### Step 5: Model with Quadratic Terms (Tipping Points)\n",
    "\n",
    "Add quadratic terms to identify tipping points where media effectiveness changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0e359",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Add quadratic terms for key media channels\n",
    "# Adjust based on actual column names in your data\n",
    "# quadratic_model <- lm(Sales ~ Radio + I(Radio^2) + TV + I(TV^2) + \n",
    "#                       Newspaper + I(Newspaper^2) + Board + I(Board^2) + \n",
    "#                       Bus + I(Bus^2) + Taxi + I(Taxi^2), data=zadas_data)\n",
    "# \n",
    "# summary(quadratic_model)\n",
    "# \n",
    "# # Identify tipping points (where derivative = 0)\n",
    "# # For a quadratic term: ax^2 + bx, tipping point is at x = -b/(2a)\n",
    "\n",
    "print(\"Quadratic model will help identify tipping points for media spending.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0dbd7",
   "metadata": {},
   "source": [
    "### Step 6: Interaction Effects Between Media Channels\n",
    "\n",
    "Examine synergistic effects between different media channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e3d76",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Test key interaction effects\n",
    "# interaction_model <- lm(Sales ~ Radio + TV + Newspaper + Board + Bus + Taxi +\n",
    "#                         Radio*TV + Radio*Newspaper + TV*Newspaper, \n",
    "#                         data=zadas_data)\n",
    "# \n",
    "# summary(interaction_model)\n",
    "\n",
    "print(\"Interaction model will reveal synergistic effects between media channels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885948cd",
   "metadata": {},
   "source": [
    "### Step 7: Model Comparison and Recommendations\n",
    "\n",
    "Compare all models and provide recommendations for resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85a321",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "# models_comparison <- data.frame(\n",
    "#   Model = c(\"Full Model\", \"Stepwise\", \"Quadratic\", \"Interaction\"),\n",
    "#   R_squared = c(summary(full_model)$r.squared,\n",
    "#                 summary(step_model)$r.squared,\n",
    "#                 summary(quadratic_model)$r.squared,\n",
    "#                 summary(interaction_model)$r.squared),\n",
    "#   Adj_R_squared = c(summary(full_model)$adj.r.squared,\n",
    "#                     summary(step_model)$adj.r.squared,\n",
    "#                     summary(quadratic_model)$adj.r.squared,\n",
    "#                     summary(interaction_model)$adj.r.squared),\n",
    "#   AIC = c(AIC(full_model), AIC(step_model), AIC(quadratic_model), AIC(interaction_model))\n",
    "# )\n",
    "# \n",
    "# print(models_comparison)\n",
    "\n",
    "print(\"Model comparison will help identify the best approach for understanding media effectiveness.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb078d6",
   "metadata": {},
   "source": [
    "### Step 8: Visualization of Media Effects\n",
    "\n",
    "Create visualizations to communicate findings about media effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d36a2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize coefficient estimates with confidence intervals\n",
    "# library(ggplot2)\n",
    "# \n",
    "# # Extract coefficients and confidence intervals from best model\n",
    "# coef_df <- data.frame(\n",
    "#   variable = names(coef(step_model))[-1],  # exclude intercept\n",
    "#   estimate = coef(step_model)[-1],\n",
    "#   confint(step_model)[-1,]\n",
    "# )\n",
    "# names(coef_df)[3:4] <- c(\"lower\", \"upper\")\n",
    "# \n",
    "# # Plot coefficient estimates\n",
    "# ggplot(coef_df, aes(x=reorder(variable, estimate), y=estimate)) +\n",
    "#   geom_point(size=3) +\n",
    "#   geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2) +\n",
    "#   geom_hline(yintercept=0, linetype=\"dashed\", color=\"red\") +\n",
    "#   coord_flip() +\n",
    "#   labs(title=\"Media Channel Effectiveness on Sales\",\n",
    "#        x=\"Media Channel\",\n",
    "#        y=\"Coefficient Estimate (Effect on Sales)\") +\n",
    "#   theme_minimal()\n",
    "\n",
    "print(\"Visualizations will help communicate the relative impact of each media channel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3ba9e",
   "metadata": {},
   "source": [
    "### Exercise Summary: Key Questions to Answer\n",
    "\n",
    "Based on your analysis, prepare answers to the following questions:\n",
    "\n",
    "1. **Media Effectiveness**: Which media channels have the strongest positive impact on sales? Which are least effective?\n",
    "\n",
    "2. **Resource Reallocation**: Based on coefficient estimates and statistical significance, should Zadas reallocate resources from less effective to more effective channels?\n",
    "\n",
    "3. **Tipping Points**: Do any media channels show diminishing returns (negative quadratic coefficients)? At what spending level?\n",
    "\n",
    "4. **Interaction Effects**: Are there synergistic effects where combining certain media channels produces greater-than-additive results?\n",
    "\n",
    "5. **Multicollinearity**: How severe is the multicollinearity problem? Did stepwise regression help address it?\n",
    "\n",
    "6. **Final Recommendations**: What is your overall recommendation for Zadas' media investment strategy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
